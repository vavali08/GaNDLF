{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GaNDLF \u00b6 The G ener a lly N uanced D eep L earning F ramework (GaNDLF) for segmentation and classification. Why use GaNDLF? \u00b6 Supports multiple Deep Learning model architectures Channels/modalities Prediction classes Robust data augmentation, courtesy of TorchIO and Albumentations Built-in cross validation, with support for parallel HPC-based computing Multi-GPU (on the same machine) training Leverages robust open source software Zero -code needed to train robust models Low -code requirement for customization Automatic mixed precision support Table of Contents \u00b6 Getting Started Application Setup Usage Customize the training and inference Extending GaNDLF FAQ Acknowledgements Citation \u00b6 Please cite the following article for GaNDLF ( full paper ): @article { pati2023gandlf , author = {Pati, Sarthak and Thakur, Siddhesh P. and Hamamc{\\i}, {\\.{I}}brahim Ethem and Baid, Ujjwal and Baheti, Bhakti and Bhalerao, Megh and G{\\\"u}ley, Orhun and Mouchtaris, Sofia and Lang, David and Thermos, Spyridon and Gotkowski, Karol and Gonz{\\'a}lez, Camila and Grenko, Caleb and Getka, Alexander and Edwards, Brandon and Sheller, Micah and Wu, Junwen and Karkada, Deepthi and Panchumarthy, Ravi and Ahluwalia, Vinayak and Zou, Chunrui and Bashyam, Vishnu and Li, Yuemeng and Haghighi, Babak and Chitalia, Rhea and Abousamra, Shahira and Kurc, Tahsin M. and Gastounioti, Aimilia and Er, Sezgin and Bergman, Mark and Saltz, Joel H. and Fan, Yong and Shah, Prashant and Mukhopadhyay, Anirban and Tsaftaris, Sotirios A. and Menze, Bjoern and Davatzikos, Christos and Kontos, Despina and Karargyris, Alexandros and Umeton, Renato and Mattson, Peter and Bakas, Spyridon} , title = {GaNDLF: the generally nuanced deep learning framework for scalable end-to-end clinical workflows} , journal = {Communications Engineering} , year = {2023} , month = {May} , day = {16} , volume = {2} , number = {1} , pages = {23} , issn = {2731-3395} , doi = {10.1038/s44172-023-00066-3} , url = {https://doi.org/10.1038/s44172-023-00066-3} } Contact \u00b6 GaNDLF developers can be reached via the following ways: GitHub Discussions Email","title":"Home"},{"location":"#gandlf","text":"The G ener a lly N uanced D eep L earning F ramework (GaNDLF) for segmentation and classification.","title":"GaNDLF"},{"location":"#why-use-gandlf","text":"Supports multiple Deep Learning model architectures Channels/modalities Prediction classes Robust data augmentation, courtesy of TorchIO and Albumentations Built-in cross validation, with support for parallel HPC-based computing Multi-GPU (on the same machine) training Leverages robust open source software Zero -code needed to train robust models Low -code requirement for customization Automatic mixed precision support","title":"Why use GaNDLF?"},{"location":"#table-of-contents","text":"Getting Started Application Setup Usage Customize the training and inference Extending GaNDLF FAQ Acknowledgements","title":"Table of Contents"},{"location":"#citation","text":"Please cite the following article for GaNDLF ( full paper ): @article { pati2023gandlf , author = {Pati, Sarthak and Thakur, Siddhesh P. and Hamamc{\\i}, {\\.{I}}brahim Ethem and Baid, Ujjwal and Baheti, Bhakti and Bhalerao, Megh and G{\\\"u}ley, Orhun and Mouchtaris, Sofia and Lang, David and Thermos, Spyridon and Gotkowski, Karol and Gonz{\\'a}lez, Camila and Grenko, Caleb and Getka, Alexander and Edwards, Brandon and Sheller, Micah and Wu, Junwen and Karkada, Deepthi and Panchumarthy, Ravi and Ahluwalia, Vinayak and Zou, Chunrui and Bashyam, Vishnu and Li, Yuemeng and Haghighi, Babak and Chitalia, Rhea and Abousamra, Shahira and Kurc, Tahsin M. and Gastounioti, Aimilia and Er, Sezgin and Bergman, Mark and Saltz, Joel H. and Fan, Yong and Shah, Prashant and Mukhopadhyay, Anirban and Tsaftaris, Sotirios A. and Menze, Bjoern and Davatzikos, Christos and Kontos, Despina and Karargyris, Alexandros and Umeton, Renato and Mattson, Peter and Bakas, Spyridon} , title = {GaNDLF: the generally nuanced deep learning framework for scalable end-to-end clinical workflows} , journal = {Communications Engineering} , year = {2023} , month = {May} , day = {16} , volume = {2} , number = {1} , pages = {23} , issn = {2731-3395} , doi = {10.1038/s44172-023-00066-3} , url = {https://doi.org/10.1038/s44172-023-00066-3} }","title":"Citation"},{"location":"#contact","text":"GaNDLF developers can be reached via the following ways: GitHub Discussions Email","title":"Contact"},{"location":"acknowledgements/","text":"Acknowledgements \u00b6 This file records the following pieces of information: All papers whose results were reproduced for the GaNDLF manuscript . All challenges (links and references) that various developers/users participated in with GaNDLF. All papers that used GaNDLF in their analysis. Papers \u00b6 Application Lead Author Link Brain Extraction Siddhesh Thakur DOI:10.1016/j.neuroimage.2020.117081 Brain Tumor Segmentation Megh Bhalerao DOI:10.1007/978-3-030-46643-5_21 Brain Age Prediction Vishnu Bashyam DOI:10.1093/brain/awaa160 Dental Segmentation At\u0131f Emre Y\u00fcksel DOI:10.1038/s41598-021-90386-1 Challenges \u00b6 Challenge Name Year Host Conference Link Pathologic Myopia 2019 ISBI palm.grand-challenge.org DigestPath 2019 MICCAI digestpath2019.grand-challenge.org Diabetic Foot Ulcer 2021 MICCAI dfu-2021.grand-challenge.org People \u00b6 All coders and developers of GaNDLF Supervisors: Aimilia Gastounioti Mark Bergman Yong Fan Anirban Mukhopadhyay Sotirios Tsaftaris Bjoern Menze Despina Kontos Christos Davatzikos Spyridon Bakas","title":"Acknowledgements"},{"location":"acknowledgements/#acknowledgements","text":"This file records the following pieces of information: All papers whose results were reproduced for the GaNDLF manuscript . All challenges (links and references) that various developers/users participated in with GaNDLF. All papers that used GaNDLF in their analysis.","title":"Acknowledgements"},{"location":"acknowledgements/#papers","text":"Application Lead Author Link Brain Extraction Siddhesh Thakur DOI:10.1016/j.neuroimage.2020.117081 Brain Tumor Segmentation Megh Bhalerao DOI:10.1007/978-3-030-46643-5_21 Brain Age Prediction Vishnu Bashyam DOI:10.1093/brain/awaa160 Dental Segmentation At\u0131f Emre Y\u00fcksel DOI:10.1038/s41598-021-90386-1","title":"Papers"},{"location":"acknowledgements/#challenges","text":"Challenge Name Year Host Conference Link Pathologic Myopia 2019 ISBI palm.grand-challenge.org DigestPath 2019 MICCAI digestpath2019.grand-challenge.org Diabetic Foot Ulcer 2021 MICCAI dfu-2021.grand-challenge.org","title":"Challenges"},{"location":"acknowledgements/#people","text":"All coders and developers of GaNDLF Supervisors: Aimilia Gastounioti Mark Bergman Yong Fan Anirban Mukhopadhyay Sotirios Tsaftaris Bjoern Menze Despina Kontos Christos Davatzikos Spyridon Bakas","title":"People"},{"location":"customize/","text":"This file contains mid-level information regarding various parameters that can be leveraged to customize the training/inference in GaNDLF. Model \u00b6 Defined under the global key model in the config file architecture : Defines the model architecture (aka \"network topology\") to be used for training. All options can be found here . Some examples are: Segmentation: Standardized 4-layer UNet with ( resunet ) and without ( unet ) residual connections, as described in this paper . Multi-layer UNet with ( resunet_multilayer ) and without ( unet_multilayer ) residual connections - this is a more general version of the standard UNet, where the number of layers can be specified by the user. UNet with Inception Blocks ( uinc ) is a variant of UNet with inception blocks, as described in this paper . UNetR ( unetr ) is a variant of UNet with transformers, as described in this paper . TransUNet ( transunet ) is a variant of UNet with transformers, as described in this paper . And many more. Classification/Regression: VGG configurations ( vgg11 , vgg13 , vgg16 , vgg19 ), as described in this paper . Our implementation allows true 3D computations (as opposed to 2D+1D convolutions). VGG configurations initialized with weights trained on ImageNet ( imagenet_vgg11 , imagenet_vgg13 , imagenet_vgg16 , imagenet_vgg19 ), as described in this paper . DenseNet configurations ( densenet121 , densenet161 , densenet169 , densenet201 , densenet264 ), as described in this paper . Our implementation allows true 3D computations (as opposed to 2D+1D convolutions). ResNet configurations ( resnet18 , resnet34 , resnet50 , resnet101 , resnet152 ), as described in this paper . Our implementation allows true 3D computations (as opposed to 2D+1D convolutions). And many more. dimension : Defines the dimensionality of convolutions, this is usually the same dimension as the input image, unless specialized processing is done to convert images to a different dimensionality (usually not recommended). For example, 2D images can be stacked to form a \"pseudo\" 3D image, and 3D images can be processed as \"slices\" as 2D images. final_layer : The final layer of model that will be used to generate the final prediction. Unless otherwise specified, it can be one of softmax or sigmoid or logits or none (the latter 2 are only used for regression tasks). class_list : The list of classes that will be used for training. This is expected to be a list of integers. For example, for a segmentation task, this can be a list of integers [0, 1, 2, 4] for the BraTS training case for all labels (background, necrosis, edema, and enhancing tumor). Additionally, different labels can be combined to perform \"combinatorial training\", such as [0, 1||4, 1||2||4, 4] , for the BraTS training to train on background, tumor core, whole tumor, and enhancing, respectively. For a classification task, this can be a list of integers [0, 1] . ignore_label_validation : This is the location of the label in class_list whose performance is to be ignored during metric calculation for validation/testing data norm_type : The type of normalization to be used. This can be either batch or instance or none . Various other options specific to architectures, such as (but not limited to): densenet models: growth_rate : how many filters to add each layer (k in paper) bn_size : multiplicative factor for number of bottle neck layers # (i.e. bn_size * k features in the bottleneck layer) drop_rate : dropout rate after each dense layer unet_multilayer and other networks that support multiple layers: depth : the number of encoder/decoder (or other types of) layers Loss function \u00b6 Defined in the loss_function parameter of the model configuration. By passing weighted_loss: True , the loss function will be weighted by the inverse of the class frequency. This parameter controls the function which the model is trained. All options can be found here . Some examples are: Segmentation: dice ( dice or dc ), dice and cross entropy ( dcce ), focal loss ( focal ), dice and focal ( dc_focal ), matthews ( mcc ) Classification/regression: mean squared error ( mse ) And many more. Metrics \u00b6 Defined in the metrics parameter of the model configuration. This parameter controls the metrics to be used for model evaluation for the training/validation/testing datasets. All options can be found here . Most of these metrics are calculated using TorchMetrics . Some examples are: Segmentation: dice ( dice and dice_per_label ), hausdorff distances ( hausdorff or hausdorff100 and hausdorff100_per_label ), hausdorff distances including on the 95th percentile of distances ( hausdorff95 and hausdorff95_per_label ) - Classification/regression: mean squared error ( mse ) calculated per sample Metrics calculated per cohort (these are automatically calculated for classification and regression): Classification: accuracy, precision, recall, f1, for the entire cohort (\"global\"), per classified class (\"per_class\"), per classified class averaged (\"per_class_average\"), per classified class weighted/balanced (\"per_class_weighted\") Regression: mean absolute error, pearson and spearman coefficients, calculated as mean, sum, or standard. Patching Strategy \u00b6 patch_size : The size of the patch to be used for training. This is expected to be a list of integers, with the length of the list being the same as the dimensionality of the input image. For example, for a 2D image, this can be [128, 128] , and for a 3D image, this can be [128, 128, 128] . patch_sampler : The sampler to be used for patch sampling during training. This can be one of uniform (the entire input image has equal weight on contributing a valid patch) or label (only the regions that have a valid ground truth segmentation label can contribute a patch). label sampler usually requires padding of the image to ensure blank patches are not inadvertently sampled; this can be controlled by the enable_padding parameter. inference_mechanism grid_aggregator_overlap : this option provides the option to strategize the grid aggregation output; should be either crop or average - https://torchio.readthedocs.io/patches/patch_inference.html#grid-aggregator patch_overlap : the amount of overlap of patches during inference in terms of pixels, defaults to 0 ; see https://torchio.readthedocs.io/patches/patch_inference.html#gridsampler for details. Data Preprocessing \u00b6 Defined in the data_preprocessing parameter of the model configuration. This parameter controls the various preprocessing functions that are applied to the entire image before the patching strategy is applied. All options can be found here . Some of the most important examples are: Intensity harmonization : GaNDLF provides multiple normalization and rescaling options to ensure intensity-level harmonization of the entire cohort. Some examples include: normalize : simple Z-score normalization normalize_positive : this performs z-score normalization only on pixels > 0 normalize_nonZero : this performs z-score normalization only on pixels != 0 normalize_nonZero_masked : this performs z-score normalization only on the region defined by the ground truth annotation rescale : simple min-max rescaling, sub-parameters include in_min_max , out_min_max , percentiles ; this option is useful to discard outliers in the intensity distribution Template-based normalization: These options take a target image as input (defined by the target sub-parameter) and perform different matching strategies to match input image(s) to this target. histogram_matching : this performs histogram matching as defined by this paper . If the target image is absent, this will perform global histogram equalization. If target is adaptive , this will perform adaptive histogram equalization . stain_normalization : these are normalization techniques specifically designed for histology images; the different options include vahadane , macenko , or ruifrok , under the extractor sub-parameter. Always needs a target image to work. Resolution harmonization : GaNDLF provides multiple resampling options to ensure resolution-level harmonization of the entire cohort. Some examples include: resample : resamples the image to the specified by the resolution sub-parameter resample_min : resamples the image to the maximum spacing defined by the resolution sub-parameter; this is useful in cohorts that have varying resolutions, but the user wants to resample to the minimum resolution for consistency resize_image : NOT RECOMMENDED ; resizes the image to the specified size resize_patch : NOT RECOMMENDED ; resizes the extracted patch to the specified size And many more. Data Augmentation \u00b6 Defined in the data_augmentation parameter of the model configuration. This parameter controls the various augmentation functions that are applied to the entire image before the patching strategy is applied. These should be defined in cognition of the task at hand (for example, RGB augmentations will not work for MRI/CT and other similar radiology images). All options can contain a probability sub-parameter, which defines the probability of the augmentation being applied to the image. When present, this will supersede the default_probability parameter. All options can be found here . Some of the most important examples are: Radiology-specific augmentations kspace : one of either ghosting or spiking is picked for augmentation. bias : applies a random bias field artefact to the input image using this function . RGB-specific augmentations colorjitter : applies the ColorJitter transform from PyTorch, has sub-parameters brightness , contrast , saturation , and hue . General-purpose augmentations Spatial transforms : they only change the resolution (and thereby, the shape) of the input image, and only apply interpolation to the intensities for consistency affine : applies a random affine transformation to the input image; for details, see this page ; has sub-parameters scales (defining the scaling ranges), degrees (defining the rotation ranges), and translation (defining the translation ranges in real-world coordinates , which is usually in mm ) elastic : applies a random elastic deformation to the input image; for details, see this page ; has sub-parameters num_control_points (defining the number of control points), locked_borders (defining the number of locked borders), max_displacement (defining the maximum displacement of the control points), num_control_points (defining the number of control points), and locked_borders (defining the number of locked borders). flip : applies a random flip to the input image; for details, see this page ; has sub-parameter axes (defining the axes to flip). rotate : applies a random rotation by 90 degrees ( rotate_90 ) or 180 degrees ( rotate_180 ), has sub-parameter axes (defining the axes to rotate). swap : applies a random swap , has sub-parameter patch_size (defining the patch size to swap), and num_iterations (number of iterations that 2 patches will be swapped). Intensity transforms : they change the intensity of the input image, but never the actual resolution or shape. motion : applies a random motion blur to the input image using this function . blur : applies a random Gaussian blur to the input image using this function l has sub-parameter std (defines the standard deviation range). noise : applies a random noise to the input image using this function ; has sub-parameters std (defines the standard deviation range) and mean (defines the mean of the noise to be added). noise_var : applies a random noise to the input image, however, the with default std = [0, 0.015 * std(image)] . anisotropic : applies random anisotropic transform to input image using this function . This changes the resolution and brings it back to its original resolution, thus applying \"real-world\" interpolation to images. Training Parameters \u00b6 These are various parameters that control the overall training process. verbose : generate verbose messages on console; generally used for debugging. batch_size : defines the batch size to be used for training. in_memory : this is to enable or disable lazy loading - setting to true reads all data once during data loading, resulting in improvements. num_epochs : defines the number of epochs to train for. patience : defines the number of epochs to wait for improvement before early stopping. learning_rate : defines the learning rate to be used for training. scheduler : defines the learning rate scheduler to be used for training, more details are here ; can take the following sub-parameters: type : triangle , triangle_modified , exp , step , reduce-on-plateau , cosineannealing , triangular , triangular2 , exp_range min_lr : defines the minimum learning rate to be used for training. max_lr : defines the maximum learning rate to be used for training. optimizer : defines the optimizer to be used for training, more details are here . nested_training : defines the number of folds to use nested training, takes testing and validation as sub-parameters, with integer values defining the number of folds to use. memory_save_mode : if enabled, resize/resample operations in data_preprocessing will save files to disk instead of directly getting read into memory as tensors Queue configuration : this defines how the queue for the input to the model is to be designed after the patching strategy has been applied, and more details are here . This takes the following sub-parameters: q_max_length : his determines the maximum number of patches that can be stored in the queue. Using a large number means that the queue needs to be filled less often, but more CPU memory is needed to store the patches. q_samples_per_volume : this determines the number of patches to extract from each volume. A small number of patches ensures a large variability in the queue, but training will be slower. q_num_workers : this determines the number subprocesses to use for data loading; '0' means main process is used, scale this according to available CPU resources. q_verbose : used to debug the queue","title":"Customize Training and Inference"},{"location":"customize/#model","text":"Defined under the global key model in the config file architecture : Defines the model architecture (aka \"network topology\") to be used for training. All options can be found here . Some examples are: Segmentation: Standardized 4-layer UNet with ( resunet ) and without ( unet ) residual connections, as described in this paper . Multi-layer UNet with ( resunet_multilayer ) and without ( unet_multilayer ) residual connections - this is a more general version of the standard UNet, where the number of layers can be specified by the user. UNet with Inception Blocks ( uinc ) is a variant of UNet with inception blocks, as described in this paper . UNetR ( unetr ) is a variant of UNet with transformers, as described in this paper . TransUNet ( transunet ) is a variant of UNet with transformers, as described in this paper . And many more. Classification/Regression: VGG configurations ( vgg11 , vgg13 , vgg16 , vgg19 ), as described in this paper . Our implementation allows true 3D computations (as opposed to 2D+1D convolutions). VGG configurations initialized with weights trained on ImageNet ( imagenet_vgg11 , imagenet_vgg13 , imagenet_vgg16 , imagenet_vgg19 ), as described in this paper . DenseNet configurations ( densenet121 , densenet161 , densenet169 , densenet201 , densenet264 ), as described in this paper . Our implementation allows true 3D computations (as opposed to 2D+1D convolutions). ResNet configurations ( resnet18 , resnet34 , resnet50 , resnet101 , resnet152 ), as described in this paper . Our implementation allows true 3D computations (as opposed to 2D+1D convolutions). And many more. dimension : Defines the dimensionality of convolutions, this is usually the same dimension as the input image, unless specialized processing is done to convert images to a different dimensionality (usually not recommended). For example, 2D images can be stacked to form a \"pseudo\" 3D image, and 3D images can be processed as \"slices\" as 2D images. final_layer : The final layer of model that will be used to generate the final prediction. Unless otherwise specified, it can be one of softmax or sigmoid or logits or none (the latter 2 are only used for regression tasks). class_list : The list of classes that will be used for training. This is expected to be a list of integers. For example, for a segmentation task, this can be a list of integers [0, 1, 2, 4] for the BraTS training case for all labels (background, necrosis, edema, and enhancing tumor). Additionally, different labels can be combined to perform \"combinatorial training\", such as [0, 1||4, 1||2||4, 4] , for the BraTS training to train on background, tumor core, whole tumor, and enhancing, respectively. For a classification task, this can be a list of integers [0, 1] . ignore_label_validation : This is the location of the label in class_list whose performance is to be ignored during metric calculation for validation/testing data norm_type : The type of normalization to be used. This can be either batch or instance or none . Various other options specific to architectures, such as (but not limited to): densenet models: growth_rate : how many filters to add each layer (k in paper) bn_size : multiplicative factor for number of bottle neck layers # (i.e. bn_size * k features in the bottleneck layer) drop_rate : dropout rate after each dense layer unet_multilayer and other networks that support multiple layers: depth : the number of encoder/decoder (or other types of) layers","title":"Model"},{"location":"customize/#loss-function","text":"Defined in the loss_function parameter of the model configuration. By passing weighted_loss: True , the loss function will be weighted by the inverse of the class frequency. This parameter controls the function which the model is trained. All options can be found here . Some examples are: Segmentation: dice ( dice or dc ), dice and cross entropy ( dcce ), focal loss ( focal ), dice and focal ( dc_focal ), matthews ( mcc ) Classification/regression: mean squared error ( mse ) And many more.","title":"Loss function"},{"location":"customize/#metrics","text":"Defined in the metrics parameter of the model configuration. This parameter controls the metrics to be used for model evaluation for the training/validation/testing datasets. All options can be found here . Most of these metrics are calculated using TorchMetrics . Some examples are: Segmentation: dice ( dice and dice_per_label ), hausdorff distances ( hausdorff or hausdorff100 and hausdorff100_per_label ), hausdorff distances including on the 95th percentile of distances ( hausdorff95 and hausdorff95_per_label ) - Classification/regression: mean squared error ( mse ) calculated per sample Metrics calculated per cohort (these are automatically calculated for classification and regression): Classification: accuracy, precision, recall, f1, for the entire cohort (\"global\"), per classified class (\"per_class\"), per classified class averaged (\"per_class_average\"), per classified class weighted/balanced (\"per_class_weighted\") Regression: mean absolute error, pearson and spearman coefficients, calculated as mean, sum, or standard.","title":"Metrics"},{"location":"customize/#patching-strategy","text":"patch_size : The size of the patch to be used for training. This is expected to be a list of integers, with the length of the list being the same as the dimensionality of the input image. For example, for a 2D image, this can be [128, 128] , and for a 3D image, this can be [128, 128, 128] . patch_sampler : The sampler to be used for patch sampling during training. This can be one of uniform (the entire input image has equal weight on contributing a valid patch) or label (only the regions that have a valid ground truth segmentation label can contribute a patch). label sampler usually requires padding of the image to ensure blank patches are not inadvertently sampled; this can be controlled by the enable_padding parameter. inference_mechanism grid_aggregator_overlap : this option provides the option to strategize the grid aggregation output; should be either crop or average - https://torchio.readthedocs.io/patches/patch_inference.html#grid-aggregator patch_overlap : the amount of overlap of patches during inference in terms of pixels, defaults to 0 ; see https://torchio.readthedocs.io/patches/patch_inference.html#gridsampler for details.","title":"Patching Strategy"},{"location":"customize/#data-preprocessing","text":"Defined in the data_preprocessing parameter of the model configuration. This parameter controls the various preprocessing functions that are applied to the entire image before the patching strategy is applied. All options can be found here . Some of the most important examples are: Intensity harmonization : GaNDLF provides multiple normalization and rescaling options to ensure intensity-level harmonization of the entire cohort. Some examples include: normalize : simple Z-score normalization normalize_positive : this performs z-score normalization only on pixels > 0 normalize_nonZero : this performs z-score normalization only on pixels != 0 normalize_nonZero_masked : this performs z-score normalization only on the region defined by the ground truth annotation rescale : simple min-max rescaling, sub-parameters include in_min_max , out_min_max , percentiles ; this option is useful to discard outliers in the intensity distribution Template-based normalization: These options take a target image as input (defined by the target sub-parameter) and perform different matching strategies to match input image(s) to this target. histogram_matching : this performs histogram matching as defined by this paper . If the target image is absent, this will perform global histogram equalization. If target is adaptive , this will perform adaptive histogram equalization . stain_normalization : these are normalization techniques specifically designed for histology images; the different options include vahadane , macenko , or ruifrok , under the extractor sub-parameter. Always needs a target image to work. Resolution harmonization : GaNDLF provides multiple resampling options to ensure resolution-level harmonization of the entire cohort. Some examples include: resample : resamples the image to the specified by the resolution sub-parameter resample_min : resamples the image to the maximum spacing defined by the resolution sub-parameter; this is useful in cohorts that have varying resolutions, but the user wants to resample to the minimum resolution for consistency resize_image : NOT RECOMMENDED ; resizes the image to the specified size resize_patch : NOT RECOMMENDED ; resizes the extracted patch to the specified size And many more.","title":"Data Preprocessing"},{"location":"customize/#data-augmentation","text":"Defined in the data_augmentation parameter of the model configuration. This parameter controls the various augmentation functions that are applied to the entire image before the patching strategy is applied. These should be defined in cognition of the task at hand (for example, RGB augmentations will not work for MRI/CT and other similar radiology images). All options can contain a probability sub-parameter, which defines the probability of the augmentation being applied to the image. When present, this will supersede the default_probability parameter. All options can be found here . Some of the most important examples are: Radiology-specific augmentations kspace : one of either ghosting or spiking is picked for augmentation. bias : applies a random bias field artefact to the input image using this function . RGB-specific augmentations colorjitter : applies the ColorJitter transform from PyTorch, has sub-parameters brightness , contrast , saturation , and hue . General-purpose augmentations Spatial transforms : they only change the resolution (and thereby, the shape) of the input image, and only apply interpolation to the intensities for consistency affine : applies a random affine transformation to the input image; for details, see this page ; has sub-parameters scales (defining the scaling ranges), degrees (defining the rotation ranges), and translation (defining the translation ranges in real-world coordinates , which is usually in mm ) elastic : applies a random elastic deformation to the input image; for details, see this page ; has sub-parameters num_control_points (defining the number of control points), locked_borders (defining the number of locked borders), max_displacement (defining the maximum displacement of the control points), num_control_points (defining the number of control points), and locked_borders (defining the number of locked borders). flip : applies a random flip to the input image; for details, see this page ; has sub-parameter axes (defining the axes to flip). rotate : applies a random rotation by 90 degrees ( rotate_90 ) or 180 degrees ( rotate_180 ), has sub-parameter axes (defining the axes to rotate). swap : applies a random swap , has sub-parameter patch_size (defining the patch size to swap), and num_iterations (number of iterations that 2 patches will be swapped). Intensity transforms : they change the intensity of the input image, but never the actual resolution or shape. motion : applies a random motion blur to the input image using this function . blur : applies a random Gaussian blur to the input image using this function l has sub-parameter std (defines the standard deviation range). noise : applies a random noise to the input image using this function ; has sub-parameters std (defines the standard deviation range) and mean (defines the mean of the noise to be added). noise_var : applies a random noise to the input image, however, the with default std = [0, 0.015 * std(image)] . anisotropic : applies random anisotropic transform to input image using this function . This changes the resolution and brings it back to its original resolution, thus applying \"real-world\" interpolation to images.","title":"Data Augmentation"},{"location":"customize/#training-parameters","text":"These are various parameters that control the overall training process. verbose : generate verbose messages on console; generally used for debugging. batch_size : defines the batch size to be used for training. in_memory : this is to enable or disable lazy loading - setting to true reads all data once during data loading, resulting in improvements. num_epochs : defines the number of epochs to train for. patience : defines the number of epochs to wait for improvement before early stopping. learning_rate : defines the learning rate to be used for training. scheduler : defines the learning rate scheduler to be used for training, more details are here ; can take the following sub-parameters: type : triangle , triangle_modified , exp , step , reduce-on-plateau , cosineannealing , triangular , triangular2 , exp_range min_lr : defines the minimum learning rate to be used for training. max_lr : defines the maximum learning rate to be used for training. optimizer : defines the optimizer to be used for training, more details are here . nested_training : defines the number of folds to use nested training, takes testing and validation as sub-parameters, with integer values defining the number of folds to use. memory_save_mode : if enabled, resize/resample operations in data_preprocessing will save files to disk instead of directly getting read into memory as tensors Queue configuration : this defines how the queue for the input to the model is to be designed after the patching strategy has been applied, and more details are here . This takes the following sub-parameters: q_max_length : his determines the maximum number of patches that can be stored in the queue. Using a large number means that the queue needs to be filled less often, but more CPU memory is needed to store the patches. q_samples_per_volume : this determines the number of patches to extract from each volume. A small number of patches ensures a large variability in the queue, but training will be slower. q_num_workers : this determines the number subprocesses to use for data loading; '0' means main process is used, scale this according to available CPU resources. q_verbose : used to debug the queue","title":"Training Parameters"},{"location":"extending/","text":"For any new feature, please ensure the corresponding option in the sample configuration is added, so that others can review/use/extend it as needed. Environment \u00b6 Before starting to work on the code-level on GaNDLF, please follow the instructions to install GaNDLF from sources . Once that's done, please verify the installation using the following command: # continue from previous shell ( venv_gandlf ) $> # you should be in the \"GaNDLF\" git repo ( venv_gandlf ) $> python ./gandlf_verifyInstall Overall Architecture \u00b6 Command-line parsing: gandlf_run Parameters from training configuration get passed as a dict via parameter parser Training Manager : Handles k-fold training Main entry point from CLI Training Function : Performs actual training Inference Manager : Handles inference functionality Main entry point from CLI Inference Function : Performs actual inference Dependency Management \u00b6 To update/change/add a dependency in setup , please ensure at least the following conditions are met: The package is being actively maintained . The new dependency is being testing against the minimum python version supported by GaNDLF (see the python_requires variable in setup ). It does not clash with any existing dependencies. Adding Models \u00b6 For details, please see README for GANDLF.models submodule . Update Tests Adding Augmentation Transformations \u00b6 Update or add dependency in setup , if appropriate. Add transformation to global_augs_dict , defined in GANDLF/data/augmentation/__init__.py Ensure probability is used as input; probability is not used for any preprocessing operations For details, please see README for GANDLF.data.augmentation submodule . Update Tests Adding Preprocessing functionality \u00b6 Update or add dependency in setup , if appropriate; see section on Dependency Management for details. All transforms should be defined by inheriting from torchio.transforms.intensity_transform.IntensityTransform . For example, please see the threshold/clip functionality in the GANDLF/data/preprocessing/threshold_and_clip.py file. Define each option in the configuration file under the correct key (again, see threshold/clip as examples) Add transformation to global_preprocessing_dict , defined in GANDLF/data/preprocessing/__init__.py For details, please see README for GANDLF.data.preprocessing submodule . Update Tests Adding Training Functionality \u00b6 Update Training Function Update Training Manager , if any training API has changed Update Tests Adding Inference Functionality \u00b6 Update Inference Function Update Inference Manager , if any inference API has changed Update Tests Update Tests \u00b6 Once you have made changes to functionality, it is imperative that the unit tests be updated to cover the new code. Please see the full testing suite for details and examples. Run Tests \u00b6 Once you have the virtual environment set up, tests can be run using the following command: # continue from previous shell ( venv_gandlf ) $> pytest --device cuda # can be cuda or cpu, defaults to cpu Any failures will be reported in the file ${GaNDLF_HOME}/testing/failures.log . Code coverage \u00b6 The code coverage for the tests can be obtained by the following command: bash # continue from previous shell ( venv_gandlf ) $> coverage run -m pytest - -device cuda ; coverage report -m","title":"Extending GaNDLF"},{"location":"extending/#environment","text":"Before starting to work on the code-level on GaNDLF, please follow the instructions to install GaNDLF from sources . Once that's done, please verify the installation using the following command: # continue from previous shell ( venv_gandlf ) $> # you should be in the \"GaNDLF\" git repo ( venv_gandlf ) $> python ./gandlf_verifyInstall","title":"Environment"},{"location":"extending/#overall-architecture","text":"Command-line parsing: gandlf_run Parameters from training configuration get passed as a dict via parameter parser Training Manager : Handles k-fold training Main entry point from CLI Training Function : Performs actual training Inference Manager : Handles inference functionality Main entry point from CLI Inference Function : Performs actual inference","title":"Overall Architecture"},{"location":"extending/#dependency-management","text":"To update/change/add a dependency in setup , please ensure at least the following conditions are met: The package is being actively maintained . The new dependency is being testing against the minimum python version supported by GaNDLF (see the python_requires variable in setup ). It does not clash with any existing dependencies.","title":"Dependency Management"},{"location":"extending/#adding-models","text":"For details, please see README for GANDLF.models submodule . Update Tests","title":"Adding Models"},{"location":"extending/#adding-augmentation-transformations","text":"Update or add dependency in setup , if appropriate. Add transformation to global_augs_dict , defined in GANDLF/data/augmentation/__init__.py Ensure probability is used as input; probability is not used for any preprocessing operations For details, please see README for GANDLF.data.augmentation submodule . Update Tests","title":"Adding Augmentation Transformations"},{"location":"extending/#adding-preprocessing-functionality","text":"Update or add dependency in setup , if appropriate; see section on Dependency Management for details. All transforms should be defined by inheriting from torchio.transforms.intensity_transform.IntensityTransform . For example, please see the threshold/clip functionality in the GANDLF/data/preprocessing/threshold_and_clip.py file. Define each option in the configuration file under the correct key (again, see threshold/clip as examples) Add transformation to global_preprocessing_dict , defined in GANDLF/data/preprocessing/__init__.py For details, please see README for GANDLF.data.preprocessing submodule . Update Tests","title":"Adding Preprocessing functionality"},{"location":"extending/#adding-training-functionality","text":"Update Training Function Update Training Manager , if any training API has changed Update Tests","title":"Adding Training Functionality"},{"location":"extending/#adding-inference-functionality","text":"Update Inference Function Update Inference Manager , if any inference API has changed Update Tests","title":"Adding Inference Functionality"},{"location":"extending/#update-tests","text":"Once you have made changes to functionality, it is imperative that the unit tests be updated to cover the new code. Please see the full testing suite for details and examples.","title":"Update Tests"},{"location":"extending/#run-tests","text":"Once you have the virtual environment set up, tests can be run using the following command: # continue from previous shell ( venv_gandlf ) $> pytest --device cuda # can be cuda or cpu, defaults to cpu Any failures will be reported in the file ${GaNDLF_HOME}/testing/failures.log .","title":"Run Tests"},{"location":"extending/#code-coverage","text":"The code coverage for the tests can be obtained by the following command: bash # continue from previous shell ( venv_gandlf ) $> coverage run -m pytest - -device cuda ; coverage report -m","title":"Code coverage"},{"location":"faq/","text":"This page contains answers to frequently asked questions about GaNDLF. Where do I start? \u00b6 The usage guide provides a good starting point for you to understand the application of GaNDLF. If you have any questions, please feel free to post a support request , and we will do our best to address it ASAP. Why do I get the error pkg_resources.DistributionNotFound: The 'GANDLF' distribution was not found ? \u00b6 This means that GaNDLF was not installed correctly. Please ensure you have followed the installation guide properly. Why is GaNDLF not working? \u00b6 Verify that the installation has been done correctly by running python ./gandlf_verifyInstall after activating the correct virtual environment. If you are still having issues, please feel free to post a support request , and we will do our best to address it ASAP. Which parts of a GaNDLF configuration are customizable? \u00b6 Virtually all of it! For more details, please see the usage guide and our extensive samples . All available options are documented in the config_all_options.yaml file . Can I run GaNDLF on a high performance computing (HPC) cluster? \u00b6 Yes, GaNDLF has successfully been run on an SGE cluster and another managed using Kubernetes. Please post a question with more details such as the type of scheduler, and so on, and we will do our best to address it. How can I track the per-epoch training performance? \u00b6 Yes, look for logs_*.csv files in the output directory. It should be arranged in accordance with the cross-validation configuration. Furthermore, it should contain separate files for each data cohort, i.e., training/validation/testing, along with the values for all requested performance metrics, which are defined per problem type. Why are my compute jobs failing with excess RAM usage? \u00b6 If you have data_preprocessing enabled, GaNDLF will load all of the resized images as tensors into memory. Depending on your dataset (resolution, size, number of modalities), this can lead to high RAM usage. To avoid this, you can enable the memory saver mode by enabling the flag memory_save_mode in the configuration. This will write the resized images into disk. How can I resume training from a previous checkpoint? \u00b6 GaNDLF allows you to resume training from a previous checkpoint in 2 ways: - By using the --resume CLI parameter in gandlf_run , only the model weights and state dictionary will be preserved, but parameters and data are taken from the new options in the CLI. This is helpful when you are updated the training data or some compatible options in the parameters. - If both --resume and --reset are False in gandlf_run , the model weights, state dictionary, and all previously saved information (parameters, training/validation/testing data) is used to resume training. How can I update GaNDLF? \u00b6 If you have installed from pip , then you can simply run pip install --upgrade gandlf to get the latest version of GaNDLF, or if you are interested in the nightly builds, then you can run pip install --upgrade --pre gandlf . If you have performed installation from sources , then you will need to do git pull from the base GaNDLF directory to get the latest master of GaNDLF. Follow this up with pip install -e . after activating the appropriate virtual environment to ensure the updates get passed through. How can I perform federated learning of my GaNDLF model? \u00b6 Please see https://mlcommons.github.io/GaNDLF/usage/#federating-your-model-using-openfl. How can I perform federated evaluation of my GaNDLF model? \u00b6 Please see https://mlcommons.github.io/GaNDLF/usage/#federating-your-model-evaluation-using-medperf. What if I have another question? \u00b6 Please post a support request .","title":"FAQ"},{"location":"faq/#where-do-i-start","text":"The usage guide provides a good starting point for you to understand the application of GaNDLF. If you have any questions, please feel free to post a support request , and we will do our best to address it ASAP.","title":"Where do I start?"},{"location":"faq/#why-do-i-get-the-error-pkg_resourcesdistributionnotfound-the-gandlf-distribution-was-not-found","text":"This means that GaNDLF was not installed correctly. Please ensure you have followed the installation guide properly.","title":"Why do I get the error pkg_resources.DistributionNotFound: The 'GANDLF' distribution was not found?"},{"location":"faq/#why-is-gandlf-not-working","text":"Verify that the installation has been done correctly by running python ./gandlf_verifyInstall after activating the correct virtual environment. If you are still having issues, please feel free to post a support request , and we will do our best to address it ASAP.","title":"Why is GaNDLF not working?"},{"location":"faq/#which-parts-of-a-gandlf-configuration-are-customizable","text":"Virtually all of it! For more details, please see the usage guide and our extensive samples . All available options are documented in the config_all_options.yaml file .","title":"Which parts of a GaNDLF configuration are customizable?"},{"location":"faq/#can-i-run-gandlf-on-a-high-performance-computing-hpc-cluster","text":"Yes, GaNDLF has successfully been run on an SGE cluster and another managed using Kubernetes. Please post a question with more details such as the type of scheduler, and so on, and we will do our best to address it.","title":"Can I run GaNDLF on a high performance computing (HPC) cluster?"},{"location":"faq/#how-can-i-track-the-per-epoch-training-performance","text":"Yes, look for logs_*.csv files in the output directory. It should be arranged in accordance with the cross-validation configuration. Furthermore, it should contain separate files for each data cohort, i.e., training/validation/testing, along with the values for all requested performance metrics, which are defined per problem type.","title":"How can I track the per-epoch training performance?"},{"location":"faq/#why-are-my-compute-jobs-failing-with-excess-ram-usage","text":"If you have data_preprocessing enabled, GaNDLF will load all of the resized images as tensors into memory. Depending on your dataset (resolution, size, number of modalities), this can lead to high RAM usage. To avoid this, you can enable the memory saver mode by enabling the flag memory_save_mode in the configuration. This will write the resized images into disk.","title":"Why are my compute jobs failing with excess RAM usage?"},{"location":"faq/#how-can-i-resume-training-from-a-previous-checkpoint","text":"GaNDLF allows you to resume training from a previous checkpoint in 2 ways: - By using the --resume CLI parameter in gandlf_run , only the model weights and state dictionary will be preserved, but parameters and data are taken from the new options in the CLI. This is helpful when you are updated the training data or some compatible options in the parameters. - If both --resume and --reset are False in gandlf_run , the model weights, state dictionary, and all previously saved information (parameters, training/validation/testing data) is used to resume training.","title":"How can I resume training from a previous checkpoint?"},{"location":"faq/#how-can-i-update-gandlf","text":"If you have installed from pip , then you can simply run pip install --upgrade gandlf to get the latest version of GaNDLF, or if you are interested in the nightly builds, then you can run pip install --upgrade --pre gandlf . If you have performed installation from sources , then you will need to do git pull from the base GaNDLF directory to get the latest master of GaNDLF. Follow this up with pip install -e . after activating the appropriate virtual environment to ensure the updates get passed through.","title":"How can I update GaNDLF?"},{"location":"faq/#how-can-i-perform-federated-learning-of-my-gandlf-model","text":"Please see https://mlcommons.github.io/GaNDLF/usage/#federating-your-model-using-openfl.","title":"How can I perform federated learning of my GaNDLF model?"},{"location":"faq/#how-can-i-perform-federated-evaluation-of-my-gandlf-model","text":"Please see https://mlcommons.github.io/GaNDLF/usage/#federating-your-model-evaluation-using-medperf.","title":"How can I perform federated evaluation of my GaNDLF model?"},{"location":"faq/#what-if-i-have-another-question","text":"Please post a support request .","title":"What if I have another question?"},{"location":"getting_started/","text":"This document will help you get started with GaNDLF using a few representative examples. Installation \u00b6 Please follow the installation instructions to install GaNDLF. When the installation is complete, you should end up with the shell that looks like the following, which indicates that the GaNDLF virtual environment has been activated: ( venv_gandlf ) $> ### subsequent commands go here Running GaNDLF with GitHub Codespaces \u00b6 Alternatively, you can launch a Codespace for GaNDLF by clicking this link: A codespace will open in a web-based version of Visual Studio Code . The dev container is fully configured with software needed for this project. Note : Dev Containers is an open spec which is supported by GitHub Codespaces and other tools . Sample Data \u00b6 Sample data will be used for our extensive automated unit tests in all examples. You can download the sample data from this link . Example of how to do this from the terminal is shown below: # continue from previous shell ( venv_gandlf ) $> wget https://upenn.box.com/shared/static/y8162xkq1zz5555ye3pwadry2m2e39bs.zip -O ./gandlf_sample_data.zip ( venv_gandlf ) $> unzip ./gandlf_sample_data.zip # this should extract a directory called `data` in the current directory The data directory content should look like the example below (for brevity, these locations shall be referred to as ${GANDLF_DATA} in the rest of the document): # continue from previous shell ( venv_gandlf ) $> ls data 2d_histo_segmentation 2d_rad_segmentation 3d_rad_segmentation # and a bunch of CSVs which can be ignored Note : When using your own data, it is vital to correctly prepare your data prior to using it for any computational task (such as AI training or inference). Segmentation \u00b6 Segmentation using 3D Radiology Images \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sample data for this task, the base location is ${GANDLF_DATA}/3d_rad_segmentation , and it will be referred to as ${GANDLF_DATA_3DRAD} in the rest of the document. Furthermore, the CSV should look like the example below (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,Label 001,${GANDLF_DATA_3DRAD}/001/image.nii.gz,${GANDLF_DATA_3DRAD}/001/mask.nii.gz 002,${GANDLF_DATA_3DRAD}/002/image.nii.gz,${GANDLF_DATA_3DRAD}/002/mask.nii.gz 003,${GANDLF_DATA_3DRAD}/003/image.nii.gz,${GANDLF_DATA_3DRAD}/003/mask.nii.gz 004,${GANDLF_DATA_3DRAD}/004/image.nii.gz,${GANDLF_DATA_3DRAD}/004/mask.nii.gz 005,${GANDLF_DATA_3DRAD}/005/image.nii.gz,${GANDLF_DATA_3DRAD}/005/mask.nii.gz 006,${GANDLF_DATA_3DRAD}/006/image.nii.gz,${GANDLF_DATA_3DRAD}/006/mask.nii.gz 007,${GANDLF_DATA_3DRAD}/007/image.nii.gz,${GANDLF_DATA_3DRAD}/007/mask.nii.gz 008,${GANDLF_DATA_3DRAD}/008/image.nii.gz,${GANDLF_DATA_3DRAD}/008/mask.nii.gz 009,${GANDLF_DATA_3DRAD}/009/image.nii.gz,${GANDLF_DATA_3DRAD}/009/mask.nii.gz 010,${GANDLF_DATA_3DRAD}/010/image.nii.gz,${GANDLF_DATA_3DRAD}/010/mask.nii.gz 3. Construct the configuration file to help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 4. Now you are ready to train your model . 5. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, just without Label or ValueToPredict headers. Segmentation using 2D Histology Images \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Extract patches/tiles from the full-size whole slide images for training. A sample configuration to extract patches is presented here : num_patches : 3 patch_size : - 1000m - 1000m Assuming the output will be stored in ${GANDLF_DATA}/histo_patches_output , you can refer to this location as ${GANDLF_DATA_HISTO_PATCHES} in the rest of the document. Construct the main data file that will be used for the entire computation cycle. The sample data for this task should be generated after the patches are extracted . It should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,Label 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_720-3344.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_720-3344_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_816-3488.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_816-3488_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_960-3376.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_960-3376_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_976-3520.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_976-3520_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1024-3216.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1024-3216_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1104-3360.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1104-3360_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1168-3104.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1168-3104_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1248-3248.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1248-3248_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1312-3056.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1312-3056_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1392-3200.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1392-3200_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_720-3344.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_720-3344_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_816-3488.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_816-3488_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_960-3376.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_960-3376_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_976-3520.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_976-3520_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1024-3216.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1024-3216_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1104-3360.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1104-3360_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1168-3104.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1168-3104_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1248-3248.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1248-3248_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1312-3056.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1312-3056_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1392-3200.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1392-3200_LM.png 5. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 6. Now you are ready to train your model . 7. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. Note : Please consider the special considerations for histology images during inference . Classification \u00b6 Classification using 3D Radiology Images \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sample data for this task, the base location is ${GANDLF_DATA}/3d_rad_segmentation , and it will be referred to as ${GANDLF_DATA_3DRAD} in the rest of the document. The CSV should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 001,${GANDLF_DATA_3DRAD}/001/image.nii.gz,0 002,${GANDLF_DATA_3DRAD}/002/image.nii.gz,1 003,${GANDLF_DATA_3DRAD}/003/image.nii.gz,0 004,${GANDLF_DATA_3DRAD}/004/image.nii.gz,2 005,${GANDLF_DATA_3DRAD}/005/image.nii.gz,0 006,${GANDLF_DATA_3DRAD}/006/image.nii.gz,1 007,${GANDLF_DATA_3DRAD}/007/image.nii.gz,0 008,${GANDLF_DATA_3DRAD}/008/image.nii.gz,2 009,${GANDLF_DATA_3DRAD}/009/image.nii.gz,0 010,${GANDLF_DATA_3DRAD}/010/image.nii.gz,1 3. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 4. Now you are ready to train your model . 5. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. Classification (patch-level) using 2D Histology Images \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Extract patches/tiles from the full-size whole slide images for training. A sample configuration to extract patches is presented here : num_patches : 3 patch_size : - 1000m - 1000m Assuming the output will be stored in ${GANDLF_DATA}/histo_patches_output , you can refer to this location as ${GANDLF_DATA_HISTO_PATCHES} in the rest of the document. Construct the main data file that will be used for the entire computation cycle. The sample data for this task should be generated after the patches are extracted . It should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_720-3344.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_816-3488.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_960-3376.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_976-3520.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1024-3216.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1104-3360.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1168-3104.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1248-3248.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1312-3056.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1392-3200.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_720-3344.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_816-3488.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_960-3376.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_976-3520.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1024-3216.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1104-3360.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1168-3104.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1248-3248.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1312-3056.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1392-3200.png,1 5. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 6. Now you are ready to train your model . 7. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. Note : Please consider the special considerations for histology images during inference . Regression \u00b6 Regression using 3D Radiology Images \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sample data for this task, the base location is ${GANDLF_DATA}/3d_rad_segmentation , and it will be referred to as ${GANDLF_DATA_3DRAD} in the rest of the document. The CSV should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 001,${GANDLF_DATA_3DRAD}/001/image.nii.gz,0.4 002,${GANDLF_DATA_3DRAD}/002/image.nii.gz,1.2 003,${GANDLF_DATA_3DRAD}/003/image.nii.gz,0.2 004,${GANDLF_DATA_3DRAD}/004/image.nii.gz,2.3 005,${GANDLF_DATA_3DRAD}/005/image.nii.gz,0.4 006,${GANDLF_DATA_3DRAD}/006/image.nii.gz,1.2 007,${GANDLF_DATA_3DRAD}/007/image.nii.gz,0.3 008,${GANDLF_DATA_3DRAD}/008/image.nii.gz,2.2 009,${GANDLF_DATA_3DRAD}/009/image.nii.gz,0.1 010,${GANDLF_DATA_3DRAD}/010/image.nii.gz,1.5 3. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 4. Now you are ready to train your model . 5. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. Regression (patch-level) using 2D Histology Images \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Extract patches/tiles from the full-size whole slide images for training. A sample configuration to extract patches is presented here : num_patches : 3 patch_size : - 1000m - 1000m Assuming the output will be stored in ${GANDLF_DATA}/histo_patches_output , you can refer to this location as ${GANDLF_DATA_HISTO_PATCHES} in the rest of the document. Construct the main data file that will be used for the entire computation cycle. The sample data for this task should be generated after the patches are extracted . It should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_720-3344.png,0.1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_816-3488.png,0.2 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_960-3376.png,0.3 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_976-3520.png,0.6 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1024-3216.png,1.5 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1104-3360.png,1.3 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1168-3104.png,1.0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1248-3248.png,1.5 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1312-3056.png,1.1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1392-3200.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_720-3344.png,0.4 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_816-3488.png,0.5 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_960-3376.png,0.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_976-3520.png,0.5 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1024-3216.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1104-3360.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1168-3104.png,0.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1248-3248.png,1.3 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1312-3056.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1392-3200.png,1.1 4. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 5. Now you are ready to train your model . 6. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. 7. Note : Please consider the special considerations for histology images during inference .","title":"Getting Started"},{"location":"getting_started/#installation","text":"Please follow the installation instructions to install GaNDLF. When the installation is complete, you should end up with the shell that looks like the following, which indicates that the GaNDLF virtual environment has been activated: ( venv_gandlf ) $> ### subsequent commands go here","title":"Installation"},{"location":"getting_started/#running-gandlf-with-github-codespaces","text":"Alternatively, you can launch a Codespace for GaNDLF by clicking this link: A codespace will open in a web-based version of Visual Studio Code . The dev container is fully configured with software needed for this project. Note : Dev Containers is an open spec which is supported by GitHub Codespaces and other tools .","title":"Running GaNDLF with GitHub Codespaces"},{"location":"getting_started/#sample-data","text":"Sample data will be used for our extensive automated unit tests in all examples. You can download the sample data from this link . Example of how to do this from the terminal is shown below: # continue from previous shell ( venv_gandlf ) $> wget https://upenn.box.com/shared/static/y8162xkq1zz5555ye3pwadry2m2e39bs.zip -O ./gandlf_sample_data.zip ( venv_gandlf ) $> unzip ./gandlf_sample_data.zip # this should extract a directory called `data` in the current directory The data directory content should look like the example below (for brevity, these locations shall be referred to as ${GANDLF_DATA} in the rest of the document): # continue from previous shell ( venv_gandlf ) $> ls data 2d_histo_segmentation 2d_rad_segmentation 3d_rad_segmentation # and a bunch of CSVs which can be ignored Note : When using your own data, it is vital to correctly prepare your data prior to using it for any computational task (such as AI training or inference).","title":"Sample Data"},{"location":"getting_started/#segmentation","text":"","title":"Segmentation"},{"location":"getting_started/#segmentation-using-3d-radiology-images","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sample data for this task, the base location is ${GANDLF_DATA}/3d_rad_segmentation , and it will be referred to as ${GANDLF_DATA_3DRAD} in the rest of the document. Furthermore, the CSV should look like the example below (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,Label 001,${GANDLF_DATA_3DRAD}/001/image.nii.gz,${GANDLF_DATA_3DRAD}/001/mask.nii.gz 002,${GANDLF_DATA_3DRAD}/002/image.nii.gz,${GANDLF_DATA_3DRAD}/002/mask.nii.gz 003,${GANDLF_DATA_3DRAD}/003/image.nii.gz,${GANDLF_DATA_3DRAD}/003/mask.nii.gz 004,${GANDLF_DATA_3DRAD}/004/image.nii.gz,${GANDLF_DATA_3DRAD}/004/mask.nii.gz 005,${GANDLF_DATA_3DRAD}/005/image.nii.gz,${GANDLF_DATA_3DRAD}/005/mask.nii.gz 006,${GANDLF_DATA_3DRAD}/006/image.nii.gz,${GANDLF_DATA_3DRAD}/006/mask.nii.gz 007,${GANDLF_DATA_3DRAD}/007/image.nii.gz,${GANDLF_DATA_3DRAD}/007/mask.nii.gz 008,${GANDLF_DATA_3DRAD}/008/image.nii.gz,${GANDLF_DATA_3DRAD}/008/mask.nii.gz 009,${GANDLF_DATA_3DRAD}/009/image.nii.gz,${GANDLF_DATA_3DRAD}/009/mask.nii.gz 010,${GANDLF_DATA_3DRAD}/010/image.nii.gz,${GANDLF_DATA_3DRAD}/010/mask.nii.gz 3. Construct the configuration file to help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 4. Now you are ready to train your model . 5. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, just without Label or ValueToPredict headers.","title":"Segmentation using 3D Radiology Images"},{"location":"getting_started/#segmentation-using-2d-histology-images","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Extract patches/tiles from the full-size whole slide images for training. A sample configuration to extract patches is presented here : num_patches : 3 patch_size : - 1000m - 1000m Assuming the output will be stored in ${GANDLF_DATA}/histo_patches_output , you can refer to this location as ${GANDLF_DATA_HISTO_PATCHES} in the rest of the document. Construct the main data file that will be used for the entire computation cycle. The sample data for this task should be generated after the patches are extracted . It should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,Label 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_720-3344.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_720-3344_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_816-3488.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_816-3488_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_960-3376.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_960-3376_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_976-3520.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_976-3520_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1024-3216.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1024-3216_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1104-3360.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1104-3360_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1168-3104.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1168-3104_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1248-3248.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1248-3248_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1312-3056.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1312-3056_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1392-3200.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1392-3200_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_720-3344.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_720-3344_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_816-3488.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_816-3488_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_960-3376.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_960-3376_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_976-3520.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_976-3520_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1024-3216.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1024-3216_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1104-3360.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1104-3360_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1168-3104.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1168-3104_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1248-3248.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1248-3248_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1312-3056.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1312-3056_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1392-3200.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1392-3200_LM.png 5. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 6. Now you are ready to train your model . 7. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. Note : Please consider the special considerations for histology images during inference .","title":"Segmentation using 2D Histology Images"},{"location":"getting_started/#classification","text":"","title":"Classification"},{"location":"getting_started/#classification-using-3d-radiology-images","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sample data for this task, the base location is ${GANDLF_DATA}/3d_rad_segmentation , and it will be referred to as ${GANDLF_DATA_3DRAD} in the rest of the document. The CSV should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 001,${GANDLF_DATA_3DRAD}/001/image.nii.gz,0 002,${GANDLF_DATA_3DRAD}/002/image.nii.gz,1 003,${GANDLF_DATA_3DRAD}/003/image.nii.gz,0 004,${GANDLF_DATA_3DRAD}/004/image.nii.gz,2 005,${GANDLF_DATA_3DRAD}/005/image.nii.gz,0 006,${GANDLF_DATA_3DRAD}/006/image.nii.gz,1 007,${GANDLF_DATA_3DRAD}/007/image.nii.gz,0 008,${GANDLF_DATA_3DRAD}/008/image.nii.gz,2 009,${GANDLF_DATA_3DRAD}/009/image.nii.gz,0 010,${GANDLF_DATA_3DRAD}/010/image.nii.gz,1 3. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 4. Now you are ready to train your model . 5. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers.","title":"Classification using 3D Radiology Images"},{"location":"getting_started/#classification-patch-level-using-2d-histology-images","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Extract patches/tiles from the full-size whole slide images for training. A sample configuration to extract patches is presented here : num_patches : 3 patch_size : - 1000m - 1000m Assuming the output will be stored in ${GANDLF_DATA}/histo_patches_output , you can refer to this location as ${GANDLF_DATA_HISTO_PATCHES} in the rest of the document. Construct the main data file that will be used for the entire computation cycle. The sample data for this task should be generated after the patches are extracted . It should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_720-3344.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_816-3488.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_960-3376.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_976-3520.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1024-3216.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1104-3360.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1168-3104.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1248-3248.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1312-3056.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1392-3200.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_720-3344.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_816-3488.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_960-3376.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_976-3520.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1024-3216.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1104-3360.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1168-3104.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1248-3248.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1312-3056.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1392-3200.png,1 5. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 6. Now you are ready to train your model . 7. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. Note : Please consider the special considerations for histology images during inference .","title":"Classification (patch-level) using 2D Histology Images"},{"location":"getting_started/#regression","text":"","title":"Regression"},{"location":"getting_started/#regression-using-3d-radiology-images","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sample data for this task, the base location is ${GANDLF_DATA}/3d_rad_segmentation , and it will be referred to as ${GANDLF_DATA_3DRAD} in the rest of the document. The CSV should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 001,${GANDLF_DATA_3DRAD}/001/image.nii.gz,0.4 002,${GANDLF_DATA_3DRAD}/002/image.nii.gz,1.2 003,${GANDLF_DATA_3DRAD}/003/image.nii.gz,0.2 004,${GANDLF_DATA_3DRAD}/004/image.nii.gz,2.3 005,${GANDLF_DATA_3DRAD}/005/image.nii.gz,0.4 006,${GANDLF_DATA_3DRAD}/006/image.nii.gz,1.2 007,${GANDLF_DATA_3DRAD}/007/image.nii.gz,0.3 008,${GANDLF_DATA_3DRAD}/008/image.nii.gz,2.2 009,${GANDLF_DATA_3DRAD}/009/image.nii.gz,0.1 010,${GANDLF_DATA_3DRAD}/010/image.nii.gz,1.5 3. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 4. Now you are ready to train your model . 5. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers.","title":"Regression using 3D Radiology Images"},{"location":"getting_started/#regression-patch-level-using-2d-histology-images","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Extract patches/tiles from the full-size whole slide images for training. A sample configuration to extract patches is presented here : num_patches : 3 patch_size : - 1000m - 1000m Assuming the output will be stored in ${GANDLF_DATA}/histo_patches_output , you can refer to this location as ${GANDLF_DATA_HISTO_PATCHES} in the rest of the document. Construct the main data file that will be used for the entire computation cycle. The sample data for this task should be generated after the patches are extracted . It should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_720-3344.png,0.1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_816-3488.png,0.2 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_960-3376.png,0.3 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_976-3520.png,0.6 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1024-3216.png,1.5 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1104-3360.png,1.3 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1168-3104.png,1.0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1248-3248.png,1.5 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1312-3056.png,1.1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1392-3200.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_720-3344.png,0.4 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_816-3488.png,0.5 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_960-3376.png,0.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_976-3520.png,0.5 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1024-3216.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1104-3360.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1168-3104.png,0.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1248-3248.png,1.3 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1312-3056.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1392-3200.png,1.1 4. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 5. Now you are ready to train your model . 6. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. 7. Note : Please consider the special considerations for histology images during inference .","title":"Regression (patch-level) using 2D Histology Images"},{"location":"setup/","text":"Setup/Installation Instructions \u00b6 Prerequisites \u00b6 Python3 with a preference for conda , and python version 3.8 (higher versions might work, but they are untested ). Knowledge of managing Python environments . The instructions below assume knowledge of the conda management system . Alternatively, you can run GaNDLF via Docker . This needs different prerequisites. See the Docker Installation section below for more information. Optional Requirements \u00b6 GPU compute (usually needed for faster training): Install appropriate drivers: NVIDIA AMD Compute toolkit appropriate for your hardware: NVIDIA: CUDA and a compatible cuDNN installed system-wide AMD: ROCm Windows: Microsoft Visual C++ 14.0 or greater . This is required for PyTorch to work on Windows. If you are using conda, you can install it using the following command for your virtual environment: conda install -c anaconda m2w64-toolchain . Installation \u00b6 Install PyTorch \u00b6 GaNDLF's primary computational foundation is built on PyTorch, and as such it supports all hardware types that PyTorch supports. Please install PyTorch for your hardware type before installing GaNDLF. See the PyTorch installation instructions for more details. An example installation using CUDA, ROCm, and CPU-only is shown below: ( base ) $> conda create -n venv_gandlf python = 3 .8 -y ( base ) $> conda activate venv_gandlf ( venv_gandlf ) $> ### subsequent commands go here ### PyTorch installation - https://pytorch.org/get-started/previous-versions/#v1131 ## CUDA 11.6 # (venv_gandlf) $> pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116 ## ROCm # (venv_gandlf) $> pip install torch==1.13.1+rocm5.2 torchvision==0.14.1+rocm5.2 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/rocm5.2 ## CPU-only # (venv_gandlf) $> pip install torch==1.13.1+cpu torchvision==0.14.1+cpu torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cpu Optional Dependencies \u00b6 The following dependencies are optional, and are needed for specific features of GaNDLF. ( venv_gandlf ) $> pip install openvino-dev == 2023 .0.1 # [OPTIONAL] to generate post-training optimized models for inference ( venv_gandlf ) $> pip install mlcube_docker # [OPTIONAL] to deploy GaNDLF models as MLCube-compliant Docker containers Install from Package Managers \u00b6 This option is recommended for most users, and allows for the quickest way to get started with GaNDLF. # continue from previous shell ( venv_gandlf ) $> pip install gandlf # this will give you the latest stable release ## you can also use conda # (venv_gandlf) $> conda install -c conda-forge gandlf -y If you are interested in running the latest version of GaNDLF, you can install the nightly build by running the following command: # continue from previous shell ( venv_gandlf ) $> pip install --pre gandlf ## you can also use conda # (venv_gandlf) $> conda install -c conda-forge/label/gandlf_dev -c conda-forge gandlf -y Install from Sources \u00b6 Use this option if you want to contribute to GaNDLF , or are interested to make other code-level changes for your own use. # continue from previous shell ( venv_gandlf ) $> git clone https://github.com/mlcommons/GaNDLF.git ( venv_gandlf ) $> cd GaNDLF ( venv_gandlf ) $> pip install -e . Docker Installation \u00b6 We provide containerized versions of GaNDLF, which allows you to run GaNDLF without worrying about installation steps or dependencies. Steps to run the Docker version of GaNDLF \u00b6 Install the Docker Engine for your platform. GaNDLF is available from GitHub Package Registry . Several platform versions are available, including support for CUDA, ROCm, and CPU-only. Choose the one that best matches your system and drivers. For example, if you want to get the bleeding-edge GaNDLF version, and you have CUDA Toolkit v11.6, run the following command: ( base ) $> docker pull ghcr.io/mlcommons/gandlf:latest-cuda116 This will download the GaNDLF image onto your machine. See the usage page for details on how to run GaNDLF in this \"dockerized\" form. Enable GPU usage from Docker (optional, Linux only) \u00b6 In order for \"dockerized\" GaNDLF to use your GPU, several steps are needed: Ensure sure that you have correct NVIDIA drivers for your GPU. Then, on Linux, follow the instructions to set up the NVIDIA Container Toolkit . This can be replicated for ROCm for AMD , by following the instructions to set up the ROCm Container Toolkit . On Windows \u00b6 On Windows, GPU and CUDA support requires either Windows 11, or (on Windows 10) to be registered for the Windows Insider program. If you meet those requirements and have current NVIDIA drivers , GPU support for Docker should work automatically . Otherwise, please try updating your Docker Desktop version. Note : We cannot provide support for the Windows Insider program or for Docker Desktop itself. Building your own GaNDLF Docker Image \u00b6 You may also build a Docker image of GaNDLF from the source repository. Just specify the Dockerfile for your preferred GPU-compute platform (or CPU): ( base ) $> git clone https://github.com/mlcommons/GaNDLF.git ( base ) $> cd GaNDLF ( base ) $> docker build -t gandlf: ${ mytagname } -f Dockerfile- ${ target_platform } . # change ${mytagname} and ${target_platform} as needed","title":"Installation"},{"location":"setup/#setupinstallation-instructions","text":"","title":"Setup/Installation Instructions"},{"location":"setup/#prerequisites","text":"Python3 with a preference for conda , and python version 3.8 (higher versions might work, but they are untested ). Knowledge of managing Python environments . The instructions below assume knowledge of the conda management system . Alternatively, you can run GaNDLF via Docker . This needs different prerequisites. See the Docker Installation section below for more information.","title":"Prerequisites"},{"location":"setup/#optional-requirements","text":"GPU compute (usually needed for faster training): Install appropriate drivers: NVIDIA AMD Compute toolkit appropriate for your hardware: NVIDIA: CUDA and a compatible cuDNN installed system-wide AMD: ROCm Windows: Microsoft Visual C++ 14.0 or greater . This is required for PyTorch to work on Windows. If you are using conda, you can install it using the following command for your virtual environment: conda install -c anaconda m2w64-toolchain .","title":"Optional Requirements"},{"location":"setup/#installation","text":"","title":"Installation"},{"location":"setup/#install-pytorch","text":"GaNDLF's primary computational foundation is built on PyTorch, and as such it supports all hardware types that PyTorch supports. Please install PyTorch for your hardware type before installing GaNDLF. See the PyTorch installation instructions for more details. An example installation using CUDA, ROCm, and CPU-only is shown below: ( base ) $> conda create -n venv_gandlf python = 3 .8 -y ( base ) $> conda activate venv_gandlf ( venv_gandlf ) $> ### subsequent commands go here ### PyTorch installation - https://pytorch.org/get-started/previous-versions/#v1131 ## CUDA 11.6 # (venv_gandlf) $> pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116 ## ROCm # (venv_gandlf) $> pip install torch==1.13.1+rocm5.2 torchvision==0.14.1+rocm5.2 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/rocm5.2 ## CPU-only # (venv_gandlf) $> pip install torch==1.13.1+cpu torchvision==0.14.1+cpu torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cpu","title":"Install PyTorch"},{"location":"setup/#optional-dependencies","text":"The following dependencies are optional, and are needed for specific features of GaNDLF. ( venv_gandlf ) $> pip install openvino-dev == 2023 .0.1 # [OPTIONAL] to generate post-training optimized models for inference ( venv_gandlf ) $> pip install mlcube_docker # [OPTIONAL] to deploy GaNDLF models as MLCube-compliant Docker containers","title":"Optional Dependencies"},{"location":"setup/#install-from-package-managers","text":"This option is recommended for most users, and allows for the quickest way to get started with GaNDLF. # continue from previous shell ( venv_gandlf ) $> pip install gandlf # this will give you the latest stable release ## you can also use conda # (venv_gandlf) $> conda install -c conda-forge gandlf -y If you are interested in running the latest version of GaNDLF, you can install the nightly build by running the following command: # continue from previous shell ( venv_gandlf ) $> pip install --pre gandlf ## you can also use conda # (venv_gandlf) $> conda install -c conda-forge/label/gandlf_dev -c conda-forge gandlf -y","title":"Install from Package Managers"},{"location":"setup/#install-from-sources","text":"Use this option if you want to contribute to GaNDLF , or are interested to make other code-level changes for your own use. # continue from previous shell ( venv_gandlf ) $> git clone https://github.com/mlcommons/GaNDLF.git ( venv_gandlf ) $> cd GaNDLF ( venv_gandlf ) $> pip install -e .","title":"Install from Sources"},{"location":"setup/#docker-installation","text":"We provide containerized versions of GaNDLF, which allows you to run GaNDLF without worrying about installation steps or dependencies.","title":"Docker Installation"},{"location":"setup/#steps-to-run-the-docker-version-of-gandlf","text":"Install the Docker Engine for your platform. GaNDLF is available from GitHub Package Registry . Several platform versions are available, including support for CUDA, ROCm, and CPU-only. Choose the one that best matches your system and drivers. For example, if you want to get the bleeding-edge GaNDLF version, and you have CUDA Toolkit v11.6, run the following command: ( base ) $> docker pull ghcr.io/mlcommons/gandlf:latest-cuda116 This will download the GaNDLF image onto your machine. See the usage page for details on how to run GaNDLF in this \"dockerized\" form.","title":"Steps to run the Docker version of GaNDLF"},{"location":"setup/#enable-gpu-usage-from-docker-optional-linux-only","text":"In order for \"dockerized\" GaNDLF to use your GPU, several steps are needed: Ensure sure that you have correct NVIDIA drivers for your GPU. Then, on Linux, follow the instructions to set up the NVIDIA Container Toolkit . This can be replicated for ROCm for AMD , by following the instructions to set up the ROCm Container Toolkit .","title":"Enable GPU usage from Docker (optional, Linux only)"},{"location":"setup/#on-windows","text":"On Windows, GPU and CUDA support requires either Windows 11, or (on Windows 10) to be registered for the Windows Insider program. If you meet those requirements and have current NVIDIA drivers , GPU support for Docker should work automatically . Otherwise, please try updating your Docker Desktop version. Note : We cannot provide support for the Windows Insider program or for Docker Desktop itself.","title":"On Windows"},{"location":"setup/#building-your-own-gandlf-docker-image","text":"You may also build a Docker image of GaNDLF from the source repository. Just specify the Dockerfile for your preferred GPU-compute platform (or CPU): ( base ) $> git clone https://github.com/mlcommons/GaNDLF.git ( base ) $> cd GaNDLF ( base ) $> docker build -t gandlf: ${ mytagname } -f Dockerfile- ${ target_platform } . # change ${mytagname} and ${target_platform} as needed","title":"Building your own GaNDLF Docker Image"},{"location":"usage/","text":"Introduction \u00b6 For any DL pipeline, the following flow needs to be performed: Data preparation Split data into training, validation, and testing Customize the training parameters A detailed data flow diagram is presented in this link . GaNDLF addresses all of these, and the information is divided as described in the following sections. Installation \u00b6 Please follow the installation instructions to install GaNDLF. When the installation is complete, you should end up with the shell that looks like the following, which indicates that the GaNDLF virtual environment has been activated: ( venv_gandlf ) $> ### subsequent commands go here Preparing the Data \u00b6 Anonymize Data \u00b6 A major reason why one would want to anonymize data is to ensure that trained models do not inadvertently do not encode protect health information [ 1 , 2 ]. GaNDLF can anonymize single images or a collection of images using the gandlf_anonymizer script. It can be used as follows: # continue from previous shell ( venv_gandlf ) $> python gandlf_anonymizer # -h, --help Show help message and exit # -v, --version Show program's version number and exit. -c ./samples/config_anonymizer.yaml \\ # anonymizer configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./input_dir_or_file \\ # input directory containing series of images to anonymize or a single image -o ./output_dir_or_file # output directory to save anonymized images or a single output image file Cleanup/Harmonize/Curate Data \u00b6 It is highly recommended that the dataset you want to train/infer on has been harmonized. The following requirements should be considered: Registration Within-modality co-registration [ 1 , 2 , 3 ]. OPTIONAL : Registration of all datasets to patient atlas, if applicable [ 1 , 2 , 3 ]. Intensity harmonization : Same intensity profile, i.e., normalization [ 4 , 5 , 6 , 7 ]. GaNDLF offers multiple options for intensity normalization, including Z-scoring, Min-Max scaling, and Histogram matching. Resolution harmonization : Ensures that the images have similar physical definitions (i.e., voxel/pixel size/resolution/spacing). An illustration of the impact of voxel size/resolution/spacing can be found here , and it is encourage to read this article to added context on how this issue impacts a deep learning pipeline. This functionality is available via GaNDLF's preprocessing module . Recommended tools for tackling all aforementioned curation and annotation tasks: - Cancer Imaging Phenomics Toolkit (CaPTk) - Federated Tumor Segmentation (FeTS) Front End - 3D Slicer - ITK-SNAP Offline Patch Extraction (for histology images only) \u00b6 GaNDLF can be used to convert a Whole Slide Image (WSI) with or without a corresponding label map to patches/tiles using GaNDLF\u2019s integrated patch miner, which would need the following files: A configuration file that dictates how the patches/tiles will be extracted. A sample configuration to extract patches is presented here . The options that the can be defined in the configuration are as follows: patch_size : defines the size of the patches to extract, should be a tuple type of integers (e.g., [256,256] ) or a string containing patch size in microns (e.g., [100m,100m] ). This parameter always needs to be specified. scale : scale at which operations such as tissue mask calculation happens; defaults to 16 . num_patches : defines the number of patches to extract, use -1 to mine until exhaustion; defaults to -1 . value_map : mapping RGB values in label image to integer values for training; defaults to None . read_type : either random or sequential (latter is more efficient); defaults to random . overlap_factor : Portion of patches that are allowed to overlap ( 0->1 ); defaults to 0.0 . num_workers : number of workers to use for patch extraction (note that this does not scale according to the number of threads available on your machine); defaults to 1 . A CSV file with the following columns: SubjectID : the ID of the subject for the WSI Channel_0 : the full path to the WSI file which will be used to extract patches Label : (optional) full path to the label map file Once these files are present, the patch miner can be run using the following command: # continue from previous shell ( venv_gandlf ) $> python gandlf_patchMiner \\ # -h, --help Show help message and exit -c ./exp_patchMiner/config.yaml \\ # patch extraction configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./exp_patchMiner/input.csv \\ # data in CSV format -o ./exp_patchMiner/output_dir/ # output directory Running preprocessing before training/inference (optional) \u00b6 Running preprocessing before training/inference is optional, but recommended. It will significantly reduce the computational footprint during training/inference at the expense of larger storage requirements. To run preprocessing before training/inference you can use the following command, which will save the processed data in ./experiment_0/output_dir/ with a new data CSV and the corresponding model configuration: # continue from previous shell ( venv_gandlf ) $> python gandlf_preprocess \\ # -h, --help Show help message and exit -c ./experiment_0/model.yaml \\ # model configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./experiment_0/train.csv \\ # data in CSV format -o ./experiment_0/output_dir/ # output directory Constructing the Data CSV \u00b6 This application can leverage multiple channels/modalities for training while using a multi-class segmentation file. The expected format is shown as an example in samples/sample_train.csv and needs to be structured with the following header format (which shows a CSV with N subjects, each having X channels/modalities that need to be processed): SubjectID,Channel_0,Channel_1,...,Channel_X,Label 001,/full/path/001/0.nii.gz,/full/path/001/1.nii.gz,...,/full/path/001/X.nii.gz,/full/path/001/segmentation.nii.gz 002,/full/path/002/0.nii.gz,/full/path/002/1.nii.gz,...,/full/path/002/X.nii.gz,/full/path/002/segmentation.nii.gz ... N,/full/path/N/0.nii.gz,/full/path/N/1.nii.gz,...,/full/path/N/X.nii.gz,/full/path/N/segmentation.nii.gz Channel can be substituted with Modality or Image Label can be substituted with Mask or Segmentation and is used to specify the annotation file for segmentation models For classification/regression, add a column called ValueToPredict . Currently, we are supporting only a single value prediction per model. Only a single Label or ValueToPredict header should be passed Multiple segmentation classes should be in a single file with unique label numbers. Multi-label classification/regression is currently not supported. Using the gandlf_constructCSV application \u00b6 To make the process of creating the CSV easier, we have provided a utility application called gandlf_constructCSV . This script works when the data is arranged in the following format (example shown of the data directory arrangement from the Brain Tumor Segmentation (BraTS) Challenge ): $DATA_DIRECTORY \u2502 \u2514\u2500\u2500\u2500Patient_001 # this is constructed from the ${PatientID} header of CSV \u2502 \u2502 Patient_001_brain_t1.nii.gz \u2502 \u2502 Patient_001_brain_t1ce.nii.gz \u2502 \u2502 Patient_001_brain_t2.nii.gz \u2502 \u2502 Patient_001_brain_flair.nii.gz \u2502 \u2502 Patient_001_seg.nii.gz # optional for segmentation tasks \u2502 \u2514\u2500\u2500\u2500Patient_002 # this is constructed from the ${PatientID} header of CSV \u2502 \u2502 Patient_002_brain_t1.nii.gz \u2502 \u2502 Patient_002_brain_t1ce.nii.gz \u2502 \u2502 Patient_002_brain_t2.nii.gz \u2502 \u2502 Patient_002_brain_flair.nii.gz \u2502 \u2502 Patient_002_seg.nii.gz # optional for segmentation tasks \u2502 \u2514\u2500\u2500\u2500JaneDoe # this is constructed from the ${PatientID} header of CSV \u2502 \u2502 randomFileName_0_t1.nii.gz # the string identifier needs to be the same for each modality \u2502 \u2502 randomFileName_1_t1ce.nii.gz \u2502 \u2502 randomFileName_2_t2.nii.gz \u2502 \u2502 randomFileName_3_flair.nii.gz \u2502 \u2502 randomFileName_seg.nii.gz # optional for segmentation tasks \u2502 ... The following command shows how the script works: # continue from previous shell ( venv_gandlf ) $> python gandlf_constructCSV \\ # -h, --help Show help message and exit -i $DATA_DIRECTORY # this is the main data directory -c _t1.nii.gz,_t1ce.nii.gz,_t2.nii.gz,_flair.nii.gz \\ # an example image identifier for 4 structural brain MR sequences for BraTS, and can be changed based on your data -l _seg.nii.gz \\ # an example label identifier - not needed for regression/classification, and can be changed based on your data -o ./experiment_0/train_data.csv # output CSV to be used for training Notes : For classification/regression, add a column called ValueToPredict . Currently, we are supporting only a single value prediction per model. SubjectID or PatientName is used to ensure that the randomized split is done per-subject rather than per-image. For data arrangement different to what is described above, a customized script will need to be written to generate the CSV, or you can enter the data manually into the CSV. Customize the Training \u00b6 GaNDLF requires a YAML-based configuration that controls various aspects of the training/inference process. There are multiple samples for users to start as their baseline for further customization. A list of the available samples is presented as follows: Sample showing all the available options Segmentation example Regression example Classification example Notes : More details on the configuration options are available in the customization page . Ensure that the configuration has valid syntax by checking the file using any YAML validator such as yamlchecker.com or yamlvalidator.com before trying to train. Running multiple experiments (optional) \u00b6 The gandlf_configGenerator script can be used to generate a grid of configurations for tuning the hyperparameters of a baseline configuration that works for your dataset and problem. Use a strategy file (example is shown in samples/config_generator_strategy.yaml . Provide the baseline configuration which has enabled you to successfully train a model for 1 epoch for your dataset and problem at hand (regardless of the efficacy). Run the following command: # continue from previous shell ( venv_gandlf ) $> python gandlf_configGenerator \\ # -h, --help Show help message and exit -c ./samples/config_all_options.yaml \\ # baseline configuration -s ./samples/config_generator_strategy.yaml \\ # strategy file -o ./all_experiments/ # output directory 5. For example, to generate 4 configurations that leverage unet and resunet architectures for learning rates of [0.1,0.01] , you can use the following strategy file: model : { architecture : [ unet , resunet ], } learning_rate : [ 0.1 , 0.01 ] Running GaNDLF (Training/Inference) \u00b6 You can use the following code snippet to run GaNDLF: # continue from previous shell ( venv_gandlf ) $> python gandlf_run \\ # -h, --help Show help message and exit # -v, --version Show program's version number and exit. -c ./experiment_0/model.yaml \\ # model configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./experiment_0/train.csv \\ # data in CSV format -m ./experiment_0/model_dir/ \\ # model directory (i.e., the `modeldir`) where the output of the training will be stored, created if not present -t True \\ # True == train, False == inference -d cuda # ensure CUDA_VISIBLE_DEVICES env variable is set for GPU device, use 'cpu' for CPU workloads # -rt , --reset # [optional] completely resets the previous run by deleting `modeldir` # -rm , --resume # [optional] resume previous training by only keeping model dict in `modeldir` Special notes for Inference for Histology images \u00b6 If you trying to perform inference on pre-extracted patches, please change the modality key in the configuration to rad . This will ensure the histology-specific pipelines are not triggered. However, if you are trying to perform inference on full WSIs, modality should be kept as histo . Generate Metrics \u00b6 GaNDLF provides a script to generate metrics after an inference process is done.The script can be used as follows: # continue from previous shell ( venv_gandlf ) $> python gandlf_generateMetrics \\ # -h, --help Show help message and exit # -v, --version Show program's version number and exit. -c , --config The configuration file ( contains all the information related to the training/inference session ) -i , --inputdata CSV file that is used to generate the metrics ; should contain 3 columns: 'subjectid, prediction, target' -o , --outputfile Location to save the output dictionary. If not provided, will print to stdout. Once you have your CSV in the specific format, you can pass it on to generate the metrics. Here is an example for segmentation: SubjectID,Target,Prediction 001,/path/to/001/target.nii.gz,/path/to/001/prediction.nii.gz 002,/path/to/002/target.nii.gz,/path/to/002/prediction.nii.gz ... Similarly for classification or regression ( A , B , C , D are integers for classification and floats for regression): SubjectID,Target,Prediction 001,A,B 002,C,D ... To generate image to image metrics for synthesis tasks (including for the BraTS synthesis tasks [ 1 , 2 ]), ensure that the config has problem_type: synthesis , and the CSV can be in the same format as segmentation (note that the Mask column is optional): SubjectID,Target,Prediction,Mask 001,/path/to/001/target_image.nii.gz,/path/to/001/prediction_image.nii.gz,/path/to/001/brain_mask.nii.gz 002,/path/to/002/target_image.nii.gz,/path/to/002/prediction_image.nii.gz,/path/to/002/brain_mask.nii.gz ... Parallelize the Training \u00b6 Multi-GPU training \u00b6 GaNDLF enables relatively straightforward multi-GPU training. Simply set the CUDA_VISIBLE_DEVICES environment variable to the list of GPUs you want to use, and pass cuda as the device to the gandlf_run script. For example, if you want to use GPUs 0, 1, and 2, you would set CUDA_VISIBLE_DEVICES=0,1,2 [ ref ] and pass -d cuda to the gandlf_run script. Distributed training \u00b6 Distributed training is a more difficult problem to address, since there are multiple ways to configure a high-performance computing cluster (SLURM, OpenHPC, Kubernetes, and so on). Owing to this discrepancy, we have ensured that GaNDLF allows multiple training jobs to be submitted in relatively straightforward manner using the command line inference of each site\u2019s configuration. Simply populate the parallel_compute_command in the configuration with the specific command to run before the training job, and GaNDLF will use this string to submit the training job. Expected Output(s) \u00b6 Training \u00b6 Once your model is trained, you should see the following output: # continue from previous shell ( venv_gandlf ) $> ls ./experiment_0/model_dir/ data_ ${ cohort_type } .csv # data CSV used for the different cohorts, which can be either training/validation/testing data_ ${ cohort_type } .pkl # same as above, but in pickle format logs_ ${ cohort_type } .csv # logs for the different cohorts that contain the various metrics, which can be either training/validation/testing ${ architecture_name } _best.pth.tar # the best model in native PyTorch format ${ architecture_name } _latest.pth.tar # the latest model in native PyTorch format ${ architecture_name } _initial.pth.tar # the initial model in native PyTorch format ${ architecture_name } _initial. { onnx/xml/bin } # [optional] if ${architecture_name} is supported, the graph-optimized best model in ONNX format # other files dependent on if training/validation/testing output was enabled in configuration Inference \u00b6 The output of inference will be predictions based on the model that was trained. The predictions will be saved in the same directory as the model if outputdir is not passed to gandlf_run . For segmentation, a directory will be created per subject ID in the input CSV. For classification/regression, the predictions will be generated in the outputdir or modeldir as a CSV file. Plot the final results \u00b6 After the testing/validation training is finished, GaNDLF enables the collection of all the statistics from the final models for testing and validation datasets and plot them. The gandlf_collectStats can be used for plotting: # continue from previous shell ( venv_gandlf ) $> python gandlf_collectStats \\ -m /path/to/trained/models \\ # directory which contains testing and validation models -o ./experiment_0/output_dir_stats/ # output directory to save stats and plot M3D-CAM usage \u00b6 The integration of the M3D-CAM library into GaNDLF enables the generation of attention maps for 3D/2D images in the validation epoch for classification and segmentation tasks. To activate M3D-CAM you just need to add the following parameter to the config: medcam : { backend : \"gcam\" , layer : \"auto\" } You can choose one of the following backends: Grad-CAM ( gcam ) Guided Backpropagation ( gbp ) Guided Grad-CAM ( ggcam ) Grad-CAM++ ( gcampp ) Optionally one can also change the name of the layer for which the attention maps should be generated. The default behavior is auto which chooses the last convolutional layer. All generated attention maps can be found in the experiment's output directory. Link to the original repository: github.com/MECLabTUDA/M3d-Cam Post-Training Model Optimization \u00b6 If you have a model previously trained using GaNDLF that you wish to run graph optimizations on, you can use the gandlf_optimize script to do so. The following command shows how it works: # continue from previous shell ( venv_gandlf ) $> python gandlf_optimizeModel \\ -m /path/to/trained/ ${ architecture_name } _best.pth.tar # directory which contains testing and validation models If ${architecture_name} is supported, the optimized model will get generated in the model directory, with the name ${architecture_name}_optimized.onnx . Deployment \u00b6 Deploy as a Model \u00b6 GaNDLF provides the ability to deploy models into easy-to-share, easy-to-use formats -- users of your model do not even need to install GaNDLF. Currently, Docker images are supported (which can be converted to Apptainer/Singularity format ). These images meet the MLCube interface . This allows your algorithm to be used in a consistent manner with other machine learning tools. The resulting image contains your specific version of GaNDLF (including any custom changes you have made) and your trained model and configuration. This ensures that upstream changes to GaNDLF will not break compatibility with your model. To deploy a model, simply run the gandlf_deploy command after training a model. You will need the Docker engine installed to build Docker images. This will create the image and, for MLCubes, generate an MLCube directory complete with an mlcube.yaml specifications file, along with the workspace directory copied from a pre-existing template. # continue from previous shell ( venv_gandlf ) $> python gandlf_deploy \\ # -h, --help Show help message and exit -c ./experiment_0/model.yaml \\ # Configuration to bundle with the model (you can recover it with gandlf_recoverConfig first if needed) -m ./experiment_0/model_dir/ \\ # model directory (i.e., modeldir) --target docker \\ # the target platform (--help will show all available targets) --mlcube-root ./my_new_mlcube_dir \\ # Directory containing mlcube.yaml (used to configure your image base) -o ./output_dir # Output directory where a new mlcube.yaml file to be distributed with your image will be created --mlcube-type model # deploy as a model MLCube. Deploy as a Metrics Generator \u00b6 You can also deploy GaNDLF as a metrics generator (see the Generate Metrics section) as follows: ( venv_gandlf ) $> python gandlf_deploy \\ ## -h, --help show help message and exit --target docker \\ # the target platform (--help will show all available targets) --mlcube-root ./my_new_mlcube_dir \\ # Directory containing mlcube.yaml (used to configure your image base) -o ./output_dir # Output directory where a new mlcube.yaml file to be distributed with your image will be created -e ./my_custom_script.py # An optional custom script used as an entrypoint for your MLCube --mlcube-type metrics # deploy as a metrics MLCube. The resulting MLCube can be used to calculate any metrics supported in GaNDLF. You can configure which metrics to be calculated by passing a GaNDLF config file when running the MLCube. For more information about using a custom entrypoint script, see the examples here . Federating your model using OpenFL \u00b6 Once you have a model definition, it is easy to perform federated learning using the Open Federated Learning (OpenFL) library . Follow the tutorial in this page to get started. Federating your model evaluation using MedPerf \u00b6 Once you have a trained model, you can perform federated evaluation using MedPerf . Follow the tutorial in this page to get started. Note Please note that in order to create a GaNDLF MLCube, for technical reasons, you need write access to the GaNDLF package. With a virtual environment this should be automatic. See the installation instructions . https://docs.medperf.org/mlcubes/gandlf_mlcube/ Running with Docker \u00b6 The usage of GaNDLF remains generally the same even from Docker, but there are a few extra considerations. Once you have pulled the GaNDLF image, it will have a tag, such as cbica/gandlf:latest-cpu . Run the following command to list your images and ensure GaNDLF is present: ( main ) $> docker image ls You can invoke docker run with the appropriate tag to run GaNDLF: ( main ) $> docker run -it --rm --name gandlf cbica/gandlf:latest-cpu ${ gandlf command and parameters go here! } Remember that arguments/options for Docker itself go before the image tag, while the command and arguments for GaNDLF go after the image tag. For more details and options, see the Docker run documentation . However, most commands that require files or directories as input or output will fail, because the container, by default, cannot read or write files on your machine for security considerations . In order to fix this, you need to mount specific locations in the filesystem . Mounting Input and Output \u00b6 The container is basically a filesystem of its own. To make your data available to the container, you will need to mount in files and directories. Generally, it is useful to mount at least input directory (as read-only) and an output directory. See the Docker bind mount instructions for more information. For example, you might run: ( main ) $> docker run -it --rm --name gandlf --volume /home/researcher/gandlf_input:/input:ro --volume /home/researcher/gandlf_output:/output cbica/gandlf:latest-cpu [ command and args go here ] Remember that the process running in the container only considers the filesystem inside the container, which is structured differently from that of your host machine. Therefore, you will need to give paths relative to the mount point destination . Additionally, any paths used internally by GaNDLF will refer to locations inside the container. This means that data CSVs produced by the gandlf_constructCSV script will need to be made from the container and with input in the same locations. Expanding on our last example: ( main ) $> docker run -it --rm --name dataprep \\ --volume /home/researcher/gandlf_input:/input:ro \\ # input data is mounted as read-only --volume /home/researcher/gandlf_output:/output \\ # output data is mounted as read-write cbica/gandlf:latest-cpu \\ # change to appropriate docker image tag gandlf_constructCSV \\ # standard construct CSV API starts --inputDir /input/data \\ --outputFile /output/data.csv \\ --channelsID _t1.nii.gz \\ --labelID _seg.nii.gz The previous command will generate a data CSV file that you can safely edit outside the container (such as by adding a ValueToPredict column). Then, you can refer to the same file when running again: ( main ) $> docker run -it --rm --name training \\ --volume /home/researcher/gandlf_input:/input:ro \\ # input data is mounted as read-only --volume /home/researcher/gandlf_output:/output \\ # output data is mounted as read-write cbica/gandlf:latest-cpu \\ # change to appropriate docker image tag gandlf_run --train True \\ # standard training API starts --config /input/config.yml \\ --inputdata /output/data.csv \\ --modeldir /output/model Special Case for Training \u00b6 Considering that you want to train on an existing model that is inside the GaNDLF container (such as in an MLCube container created by gandlf_deploy ), the output will be to a location embedded inside the container. Since you cannot mount something into that spot without overwriting the model, you can instead use the built-in docker cp command to extract the model afterward. For example, you can fine-tune a model on your own data using the following commands as a starting point: # Run training on your new data ( main ) $> docker run --name gandlf_training mlcommons/gandlf-pretrained:0.0.1 -v /my/input/data:/input gandlf_run -m /embedded_model/ [ ... ] # Do not include \"--rm\" option! # Copy the finetuned model out of the container, to a location on the host ( main ) $> docker cp gandlf_training:/embedded_model /home/researcher/extracted_model # Now you can remove the container to clean up ( main ) $> docker rm -f gandlf_training Enabling GPUs \u00b6 Some special arguments need to be passed to Docker to enable it to use your GPU. With Docker version > 19.03 You can use docker run --gpus all to expose all GPUs to the container. See the NVIDIA Docker documentation for more details. If using CUDA, GaNDLF also expects the environment variable CUDA_VISIBLE_DEVICES to be set. To use the same settings as your host machine, simply add -e CUDA_VISIBLE_DEVICES to your docker run command. For example: For example: ( main ) $> docker run --gpus all -e CUDA_VISIBLE_DEVICES -it --rm --name gandlf cbica/gandlf:latest-cuda113 gandlf_run --device cuda [ ... ] This can be replicated for ROCm for AMD , by following the instructions to set up the ROCm Container Toolkit . MLCubes \u00b6 GaNDLF, and GaNDLF-created models, may be distributed as an MLCube . This involves distributing an mlcube.yaml file. That file can be specified when using the MLCube runners . The runner will perform many aspects of configuring your container for you. Currently, only the mlcube_docker runner is supported. See the MLCube documentation for more details.","title":"Usage"},{"location":"usage/#introduction","text":"For any DL pipeline, the following flow needs to be performed: Data preparation Split data into training, validation, and testing Customize the training parameters A detailed data flow diagram is presented in this link . GaNDLF addresses all of these, and the information is divided as described in the following sections.","title":"Introduction"},{"location":"usage/#installation","text":"Please follow the installation instructions to install GaNDLF. When the installation is complete, you should end up with the shell that looks like the following, which indicates that the GaNDLF virtual environment has been activated: ( venv_gandlf ) $> ### subsequent commands go here","title":"Installation"},{"location":"usage/#preparing-the-data","text":"","title":"Preparing the Data"},{"location":"usage/#anonymize-data","text":"A major reason why one would want to anonymize data is to ensure that trained models do not inadvertently do not encode protect health information [ 1 , 2 ]. GaNDLF can anonymize single images or a collection of images using the gandlf_anonymizer script. It can be used as follows: # continue from previous shell ( venv_gandlf ) $> python gandlf_anonymizer # -h, --help Show help message and exit # -v, --version Show program's version number and exit. -c ./samples/config_anonymizer.yaml \\ # anonymizer configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./input_dir_or_file \\ # input directory containing series of images to anonymize or a single image -o ./output_dir_or_file # output directory to save anonymized images or a single output image file","title":"Anonymize Data"},{"location":"usage/#cleanupharmonizecurate-data","text":"It is highly recommended that the dataset you want to train/infer on has been harmonized. The following requirements should be considered: Registration Within-modality co-registration [ 1 , 2 , 3 ]. OPTIONAL : Registration of all datasets to patient atlas, if applicable [ 1 , 2 , 3 ]. Intensity harmonization : Same intensity profile, i.e., normalization [ 4 , 5 , 6 , 7 ]. GaNDLF offers multiple options for intensity normalization, including Z-scoring, Min-Max scaling, and Histogram matching. Resolution harmonization : Ensures that the images have similar physical definitions (i.e., voxel/pixel size/resolution/spacing). An illustration of the impact of voxel size/resolution/spacing can be found here , and it is encourage to read this article to added context on how this issue impacts a deep learning pipeline. This functionality is available via GaNDLF's preprocessing module . Recommended tools for tackling all aforementioned curation and annotation tasks: - Cancer Imaging Phenomics Toolkit (CaPTk) - Federated Tumor Segmentation (FeTS) Front End - 3D Slicer - ITK-SNAP","title":"Cleanup/Harmonize/Curate Data"},{"location":"usage/#offline-patch-extraction-for-histology-images-only","text":"GaNDLF can be used to convert a Whole Slide Image (WSI) with or without a corresponding label map to patches/tiles using GaNDLF\u2019s integrated patch miner, which would need the following files: A configuration file that dictates how the patches/tiles will be extracted. A sample configuration to extract patches is presented here . The options that the can be defined in the configuration are as follows: patch_size : defines the size of the patches to extract, should be a tuple type of integers (e.g., [256,256] ) or a string containing patch size in microns (e.g., [100m,100m] ). This parameter always needs to be specified. scale : scale at which operations such as tissue mask calculation happens; defaults to 16 . num_patches : defines the number of patches to extract, use -1 to mine until exhaustion; defaults to -1 . value_map : mapping RGB values in label image to integer values for training; defaults to None . read_type : either random or sequential (latter is more efficient); defaults to random . overlap_factor : Portion of patches that are allowed to overlap ( 0->1 ); defaults to 0.0 . num_workers : number of workers to use for patch extraction (note that this does not scale according to the number of threads available on your machine); defaults to 1 . A CSV file with the following columns: SubjectID : the ID of the subject for the WSI Channel_0 : the full path to the WSI file which will be used to extract patches Label : (optional) full path to the label map file Once these files are present, the patch miner can be run using the following command: # continue from previous shell ( venv_gandlf ) $> python gandlf_patchMiner \\ # -h, --help Show help message and exit -c ./exp_patchMiner/config.yaml \\ # patch extraction configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./exp_patchMiner/input.csv \\ # data in CSV format -o ./exp_patchMiner/output_dir/ # output directory","title":"Offline Patch Extraction (for histology images only)"},{"location":"usage/#running-preprocessing-before-traininginference-optional","text":"Running preprocessing before training/inference is optional, but recommended. It will significantly reduce the computational footprint during training/inference at the expense of larger storage requirements. To run preprocessing before training/inference you can use the following command, which will save the processed data in ./experiment_0/output_dir/ with a new data CSV and the corresponding model configuration: # continue from previous shell ( venv_gandlf ) $> python gandlf_preprocess \\ # -h, --help Show help message and exit -c ./experiment_0/model.yaml \\ # model configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./experiment_0/train.csv \\ # data in CSV format -o ./experiment_0/output_dir/ # output directory","title":"Running preprocessing before training/inference (optional)"},{"location":"usage/#constructing-the-data-csv","text":"This application can leverage multiple channels/modalities for training while using a multi-class segmentation file. The expected format is shown as an example in samples/sample_train.csv and needs to be structured with the following header format (which shows a CSV with N subjects, each having X channels/modalities that need to be processed): SubjectID,Channel_0,Channel_1,...,Channel_X,Label 001,/full/path/001/0.nii.gz,/full/path/001/1.nii.gz,...,/full/path/001/X.nii.gz,/full/path/001/segmentation.nii.gz 002,/full/path/002/0.nii.gz,/full/path/002/1.nii.gz,...,/full/path/002/X.nii.gz,/full/path/002/segmentation.nii.gz ... N,/full/path/N/0.nii.gz,/full/path/N/1.nii.gz,...,/full/path/N/X.nii.gz,/full/path/N/segmentation.nii.gz Channel can be substituted with Modality or Image Label can be substituted with Mask or Segmentation and is used to specify the annotation file for segmentation models For classification/regression, add a column called ValueToPredict . Currently, we are supporting only a single value prediction per model. Only a single Label or ValueToPredict header should be passed Multiple segmentation classes should be in a single file with unique label numbers. Multi-label classification/regression is currently not supported.","title":"Constructing the Data CSV"},{"location":"usage/#using-the-gandlf_constructcsv-application","text":"To make the process of creating the CSV easier, we have provided a utility application called gandlf_constructCSV . This script works when the data is arranged in the following format (example shown of the data directory arrangement from the Brain Tumor Segmentation (BraTS) Challenge ): $DATA_DIRECTORY \u2502 \u2514\u2500\u2500\u2500Patient_001 # this is constructed from the ${PatientID} header of CSV \u2502 \u2502 Patient_001_brain_t1.nii.gz \u2502 \u2502 Patient_001_brain_t1ce.nii.gz \u2502 \u2502 Patient_001_brain_t2.nii.gz \u2502 \u2502 Patient_001_brain_flair.nii.gz \u2502 \u2502 Patient_001_seg.nii.gz # optional for segmentation tasks \u2502 \u2514\u2500\u2500\u2500Patient_002 # this is constructed from the ${PatientID} header of CSV \u2502 \u2502 Patient_002_brain_t1.nii.gz \u2502 \u2502 Patient_002_brain_t1ce.nii.gz \u2502 \u2502 Patient_002_brain_t2.nii.gz \u2502 \u2502 Patient_002_brain_flair.nii.gz \u2502 \u2502 Patient_002_seg.nii.gz # optional for segmentation tasks \u2502 \u2514\u2500\u2500\u2500JaneDoe # this is constructed from the ${PatientID} header of CSV \u2502 \u2502 randomFileName_0_t1.nii.gz # the string identifier needs to be the same for each modality \u2502 \u2502 randomFileName_1_t1ce.nii.gz \u2502 \u2502 randomFileName_2_t2.nii.gz \u2502 \u2502 randomFileName_3_flair.nii.gz \u2502 \u2502 randomFileName_seg.nii.gz # optional for segmentation tasks \u2502 ... The following command shows how the script works: # continue from previous shell ( venv_gandlf ) $> python gandlf_constructCSV \\ # -h, --help Show help message and exit -i $DATA_DIRECTORY # this is the main data directory -c _t1.nii.gz,_t1ce.nii.gz,_t2.nii.gz,_flair.nii.gz \\ # an example image identifier for 4 structural brain MR sequences for BraTS, and can be changed based on your data -l _seg.nii.gz \\ # an example label identifier - not needed for regression/classification, and can be changed based on your data -o ./experiment_0/train_data.csv # output CSV to be used for training Notes : For classification/regression, add a column called ValueToPredict . Currently, we are supporting only a single value prediction per model. SubjectID or PatientName is used to ensure that the randomized split is done per-subject rather than per-image. For data arrangement different to what is described above, a customized script will need to be written to generate the CSV, or you can enter the data manually into the CSV.","title":"Using the gandlf_constructCSV application"},{"location":"usage/#customize-the-training","text":"GaNDLF requires a YAML-based configuration that controls various aspects of the training/inference process. There are multiple samples for users to start as their baseline for further customization. A list of the available samples is presented as follows: Sample showing all the available options Segmentation example Regression example Classification example Notes : More details on the configuration options are available in the customization page . Ensure that the configuration has valid syntax by checking the file using any YAML validator such as yamlchecker.com or yamlvalidator.com before trying to train.","title":"Customize the Training"},{"location":"usage/#running-multiple-experiments-optional","text":"The gandlf_configGenerator script can be used to generate a grid of configurations for tuning the hyperparameters of a baseline configuration that works for your dataset and problem. Use a strategy file (example is shown in samples/config_generator_strategy.yaml . Provide the baseline configuration which has enabled you to successfully train a model for 1 epoch for your dataset and problem at hand (regardless of the efficacy). Run the following command: # continue from previous shell ( venv_gandlf ) $> python gandlf_configGenerator \\ # -h, --help Show help message and exit -c ./samples/config_all_options.yaml \\ # baseline configuration -s ./samples/config_generator_strategy.yaml \\ # strategy file -o ./all_experiments/ # output directory 5. For example, to generate 4 configurations that leverage unet and resunet architectures for learning rates of [0.1,0.01] , you can use the following strategy file: model : { architecture : [ unet , resunet ], } learning_rate : [ 0.1 , 0.01 ]","title":"Running multiple experiments (optional)"},{"location":"usage/#running-gandlf-traininginference","text":"You can use the following code snippet to run GaNDLF: # continue from previous shell ( venv_gandlf ) $> python gandlf_run \\ # -h, --help Show help message and exit # -v, --version Show program's version number and exit. -c ./experiment_0/model.yaml \\ # model configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./experiment_0/train.csv \\ # data in CSV format -m ./experiment_0/model_dir/ \\ # model directory (i.e., the `modeldir`) where the output of the training will be stored, created if not present -t True \\ # True == train, False == inference -d cuda # ensure CUDA_VISIBLE_DEVICES env variable is set for GPU device, use 'cpu' for CPU workloads # -rt , --reset # [optional] completely resets the previous run by deleting `modeldir` # -rm , --resume # [optional] resume previous training by only keeping model dict in `modeldir`","title":"Running GaNDLF (Training/Inference)"},{"location":"usage/#special-notes-for-inference-for-histology-images","text":"If you trying to perform inference on pre-extracted patches, please change the modality key in the configuration to rad . This will ensure the histology-specific pipelines are not triggered. However, if you are trying to perform inference on full WSIs, modality should be kept as histo .","title":"Special notes for Inference for Histology images"},{"location":"usage/#generate-metrics","text":"GaNDLF provides a script to generate metrics after an inference process is done.The script can be used as follows: # continue from previous shell ( venv_gandlf ) $> python gandlf_generateMetrics \\ # -h, --help Show help message and exit # -v, --version Show program's version number and exit. -c , --config The configuration file ( contains all the information related to the training/inference session ) -i , --inputdata CSV file that is used to generate the metrics ; should contain 3 columns: 'subjectid, prediction, target' -o , --outputfile Location to save the output dictionary. If not provided, will print to stdout. Once you have your CSV in the specific format, you can pass it on to generate the metrics. Here is an example for segmentation: SubjectID,Target,Prediction 001,/path/to/001/target.nii.gz,/path/to/001/prediction.nii.gz 002,/path/to/002/target.nii.gz,/path/to/002/prediction.nii.gz ... Similarly for classification or regression ( A , B , C , D are integers for classification and floats for regression): SubjectID,Target,Prediction 001,A,B 002,C,D ... To generate image to image metrics for synthesis tasks (including for the BraTS synthesis tasks [ 1 , 2 ]), ensure that the config has problem_type: synthesis , and the CSV can be in the same format as segmentation (note that the Mask column is optional): SubjectID,Target,Prediction,Mask 001,/path/to/001/target_image.nii.gz,/path/to/001/prediction_image.nii.gz,/path/to/001/brain_mask.nii.gz 002,/path/to/002/target_image.nii.gz,/path/to/002/prediction_image.nii.gz,/path/to/002/brain_mask.nii.gz ...","title":"Generate Metrics"},{"location":"usage/#parallelize-the-training","text":"","title":"Parallelize the Training"},{"location":"usage/#multi-gpu-training","text":"GaNDLF enables relatively straightforward multi-GPU training. Simply set the CUDA_VISIBLE_DEVICES environment variable to the list of GPUs you want to use, and pass cuda as the device to the gandlf_run script. For example, if you want to use GPUs 0, 1, and 2, you would set CUDA_VISIBLE_DEVICES=0,1,2 [ ref ] and pass -d cuda to the gandlf_run script.","title":"Multi-GPU training"},{"location":"usage/#distributed-training","text":"Distributed training is a more difficult problem to address, since there are multiple ways to configure a high-performance computing cluster (SLURM, OpenHPC, Kubernetes, and so on). Owing to this discrepancy, we have ensured that GaNDLF allows multiple training jobs to be submitted in relatively straightforward manner using the command line inference of each site\u2019s configuration. Simply populate the parallel_compute_command in the configuration with the specific command to run before the training job, and GaNDLF will use this string to submit the training job.","title":"Distributed training"},{"location":"usage/#expected-outputs","text":"","title":"Expected Output(s)"},{"location":"usage/#training","text":"Once your model is trained, you should see the following output: # continue from previous shell ( venv_gandlf ) $> ls ./experiment_0/model_dir/ data_ ${ cohort_type } .csv # data CSV used for the different cohorts, which can be either training/validation/testing data_ ${ cohort_type } .pkl # same as above, but in pickle format logs_ ${ cohort_type } .csv # logs for the different cohorts that contain the various metrics, which can be either training/validation/testing ${ architecture_name } _best.pth.tar # the best model in native PyTorch format ${ architecture_name } _latest.pth.tar # the latest model in native PyTorch format ${ architecture_name } _initial.pth.tar # the initial model in native PyTorch format ${ architecture_name } _initial. { onnx/xml/bin } # [optional] if ${architecture_name} is supported, the graph-optimized best model in ONNX format # other files dependent on if training/validation/testing output was enabled in configuration","title":"Training"},{"location":"usage/#inference","text":"The output of inference will be predictions based on the model that was trained. The predictions will be saved in the same directory as the model if outputdir is not passed to gandlf_run . For segmentation, a directory will be created per subject ID in the input CSV. For classification/regression, the predictions will be generated in the outputdir or modeldir as a CSV file.","title":"Inference"},{"location":"usage/#plot-the-final-results","text":"After the testing/validation training is finished, GaNDLF enables the collection of all the statistics from the final models for testing and validation datasets and plot them. The gandlf_collectStats can be used for plotting: # continue from previous shell ( venv_gandlf ) $> python gandlf_collectStats \\ -m /path/to/trained/models \\ # directory which contains testing and validation models -o ./experiment_0/output_dir_stats/ # output directory to save stats and plot","title":"Plot the final results"},{"location":"usage/#m3d-cam-usage","text":"The integration of the M3D-CAM library into GaNDLF enables the generation of attention maps for 3D/2D images in the validation epoch for classification and segmentation tasks. To activate M3D-CAM you just need to add the following parameter to the config: medcam : { backend : \"gcam\" , layer : \"auto\" } You can choose one of the following backends: Grad-CAM ( gcam ) Guided Backpropagation ( gbp ) Guided Grad-CAM ( ggcam ) Grad-CAM++ ( gcampp ) Optionally one can also change the name of the layer for which the attention maps should be generated. The default behavior is auto which chooses the last convolutional layer. All generated attention maps can be found in the experiment's output directory. Link to the original repository: github.com/MECLabTUDA/M3d-Cam","title":"M3D-CAM usage"},{"location":"usage/#post-training-model-optimization","text":"If you have a model previously trained using GaNDLF that you wish to run graph optimizations on, you can use the gandlf_optimize script to do so. The following command shows how it works: # continue from previous shell ( venv_gandlf ) $> python gandlf_optimizeModel \\ -m /path/to/trained/ ${ architecture_name } _best.pth.tar # directory which contains testing and validation models If ${architecture_name} is supported, the optimized model will get generated in the model directory, with the name ${architecture_name}_optimized.onnx .","title":"Post-Training Model Optimization"},{"location":"usage/#deployment","text":"","title":"Deployment"},{"location":"usage/#deploy-as-a-model","text":"GaNDLF provides the ability to deploy models into easy-to-share, easy-to-use formats -- users of your model do not even need to install GaNDLF. Currently, Docker images are supported (which can be converted to Apptainer/Singularity format ). These images meet the MLCube interface . This allows your algorithm to be used in a consistent manner with other machine learning tools. The resulting image contains your specific version of GaNDLF (including any custom changes you have made) and your trained model and configuration. This ensures that upstream changes to GaNDLF will not break compatibility with your model. To deploy a model, simply run the gandlf_deploy command after training a model. You will need the Docker engine installed to build Docker images. This will create the image and, for MLCubes, generate an MLCube directory complete with an mlcube.yaml specifications file, along with the workspace directory copied from a pre-existing template. # continue from previous shell ( venv_gandlf ) $> python gandlf_deploy \\ # -h, --help Show help message and exit -c ./experiment_0/model.yaml \\ # Configuration to bundle with the model (you can recover it with gandlf_recoverConfig first if needed) -m ./experiment_0/model_dir/ \\ # model directory (i.e., modeldir) --target docker \\ # the target platform (--help will show all available targets) --mlcube-root ./my_new_mlcube_dir \\ # Directory containing mlcube.yaml (used to configure your image base) -o ./output_dir # Output directory where a new mlcube.yaml file to be distributed with your image will be created --mlcube-type model # deploy as a model MLCube.","title":"Deploy as a Model"},{"location":"usage/#deploy-as-a-metrics-generator","text":"You can also deploy GaNDLF as a metrics generator (see the Generate Metrics section) as follows: ( venv_gandlf ) $> python gandlf_deploy \\ ## -h, --help show help message and exit --target docker \\ # the target platform (--help will show all available targets) --mlcube-root ./my_new_mlcube_dir \\ # Directory containing mlcube.yaml (used to configure your image base) -o ./output_dir # Output directory where a new mlcube.yaml file to be distributed with your image will be created -e ./my_custom_script.py # An optional custom script used as an entrypoint for your MLCube --mlcube-type metrics # deploy as a metrics MLCube. The resulting MLCube can be used to calculate any metrics supported in GaNDLF. You can configure which metrics to be calculated by passing a GaNDLF config file when running the MLCube. For more information about using a custom entrypoint script, see the examples here .","title":"Deploy as a Metrics Generator"},{"location":"usage/#federating-your-model-using-openfl","text":"Once you have a model definition, it is easy to perform federated learning using the Open Federated Learning (OpenFL) library . Follow the tutorial in this page to get started.","title":"Federating your model using OpenFL"},{"location":"usage/#federating-your-model-evaluation-using-medperf","text":"Once you have a trained model, you can perform federated evaluation using MedPerf . Follow the tutorial in this page to get started. Note Please note that in order to create a GaNDLF MLCube, for technical reasons, you need write access to the GaNDLF package. With a virtual environment this should be automatic. See the installation instructions . https://docs.medperf.org/mlcubes/gandlf_mlcube/","title":"Federating your model evaluation using MedPerf"},{"location":"usage/#running-with-docker","text":"The usage of GaNDLF remains generally the same even from Docker, but there are a few extra considerations. Once you have pulled the GaNDLF image, it will have a tag, such as cbica/gandlf:latest-cpu . Run the following command to list your images and ensure GaNDLF is present: ( main ) $> docker image ls You can invoke docker run with the appropriate tag to run GaNDLF: ( main ) $> docker run -it --rm --name gandlf cbica/gandlf:latest-cpu ${ gandlf command and parameters go here! } Remember that arguments/options for Docker itself go before the image tag, while the command and arguments for GaNDLF go after the image tag. For more details and options, see the Docker run documentation . However, most commands that require files or directories as input or output will fail, because the container, by default, cannot read or write files on your machine for security considerations . In order to fix this, you need to mount specific locations in the filesystem .","title":"Running with Docker"},{"location":"usage/#mounting-input-and-output","text":"The container is basically a filesystem of its own. To make your data available to the container, you will need to mount in files and directories. Generally, it is useful to mount at least input directory (as read-only) and an output directory. See the Docker bind mount instructions for more information. For example, you might run: ( main ) $> docker run -it --rm --name gandlf --volume /home/researcher/gandlf_input:/input:ro --volume /home/researcher/gandlf_output:/output cbica/gandlf:latest-cpu [ command and args go here ] Remember that the process running in the container only considers the filesystem inside the container, which is structured differently from that of your host machine. Therefore, you will need to give paths relative to the mount point destination . Additionally, any paths used internally by GaNDLF will refer to locations inside the container. This means that data CSVs produced by the gandlf_constructCSV script will need to be made from the container and with input in the same locations. Expanding on our last example: ( main ) $> docker run -it --rm --name dataprep \\ --volume /home/researcher/gandlf_input:/input:ro \\ # input data is mounted as read-only --volume /home/researcher/gandlf_output:/output \\ # output data is mounted as read-write cbica/gandlf:latest-cpu \\ # change to appropriate docker image tag gandlf_constructCSV \\ # standard construct CSV API starts --inputDir /input/data \\ --outputFile /output/data.csv \\ --channelsID _t1.nii.gz \\ --labelID _seg.nii.gz The previous command will generate a data CSV file that you can safely edit outside the container (such as by adding a ValueToPredict column). Then, you can refer to the same file when running again: ( main ) $> docker run -it --rm --name training \\ --volume /home/researcher/gandlf_input:/input:ro \\ # input data is mounted as read-only --volume /home/researcher/gandlf_output:/output \\ # output data is mounted as read-write cbica/gandlf:latest-cpu \\ # change to appropriate docker image tag gandlf_run --train True \\ # standard training API starts --config /input/config.yml \\ --inputdata /output/data.csv \\ --modeldir /output/model","title":"Mounting Input and Output"},{"location":"usage/#special-case-for-training","text":"Considering that you want to train on an existing model that is inside the GaNDLF container (such as in an MLCube container created by gandlf_deploy ), the output will be to a location embedded inside the container. Since you cannot mount something into that spot without overwriting the model, you can instead use the built-in docker cp command to extract the model afterward. For example, you can fine-tune a model on your own data using the following commands as a starting point: # Run training on your new data ( main ) $> docker run --name gandlf_training mlcommons/gandlf-pretrained:0.0.1 -v /my/input/data:/input gandlf_run -m /embedded_model/ [ ... ] # Do not include \"--rm\" option! # Copy the finetuned model out of the container, to a location on the host ( main ) $> docker cp gandlf_training:/embedded_model /home/researcher/extracted_model # Now you can remove the container to clean up ( main ) $> docker rm -f gandlf_training","title":"Special Case for Training"},{"location":"usage/#enabling-gpus","text":"Some special arguments need to be passed to Docker to enable it to use your GPU. With Docker version > 19.03 You can use docker run --gpus all to expose all GPUs to the container. See the NVIDIA Docker documentation for more details. If using CUDA, GaNDLF also expects the environment variable CUDA_VISIBLE_DEVICES to be set. To use the same settings as your host machine, simply add -e CUDA_VISIBLE_DEVICES to your docker run command. For example: For example: ( main ) $> docker run --gpus all -e CUDA_VISIBLE_DEVICES -it --rm --name gandlf cbica/gandlf:latest-cuda113 gandlf_run --device cuda [ ... ] This can be replicated for ROCm for AMD , by following the instructions to set up the ROCm Container Toolkit .","title":"Enabling GPUs"},{"location":"usage/#mlcubes","text":"GaNDLF, and GaNDLF-created models, may be distributed as an MLCube . This involves distributing an mlcube.yaml file. That file can be specified when using the MLCube runners . The runner will perform many aspects of configuring your container for you. Currently, only the mlcube_docker runner is supported. See the MLCube documentation for more details.","title":"MLCubes"}]}